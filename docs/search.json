[
  {
    "objectID": "tutorials/summarizing-data-tidy/index.html",
    "href": "tutorials/summarizing-data-tidy/index.html",
    "title": "Summarizing Data with dplyr",
    "section": "",
    "text": "The tidyverse offers a coherent and expressive way to compute descriptive statistics. Instead of calling separate functions on individual vectors, tidyverse tools let you describe an entire dataset through clear workflows built with pipelines. The basic idea is always the same, to start with your data, then add layers of transformations one step at a time.\nIn this section, we explore how to use dplyr to compute summary statistics, work with multiple variables at once, handle missing values, and produce grouped summaries.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data with tidyverse"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-tidy/na_complete.html",
    "href": "tutorials/summarizing-data-tidy/na_complete.html",
    "title": "Counting and Missing Values",
    "section": "",
    "text": "The tidyverse includes simple helpers for counting observations, unique values, and missing data. These are especially useful for quick checks.\nIf you just want to know how many rows your dataset contains, you can use n() inside summarise():\n\nlibrary(dplyr)\nstocks %&gt;% summarise(n = n())\n\n   n\n1 88\n\n\nTo look at missing data, we combine is.na() with sum(). This lets us count all the missing values in a particular column, making the amount of missingness explicit:\n\nstocks %&gt;% summarise(missing = sum(is.na(TBONDS)))\n\n  missing\n1       6\n\n\nSimilarly, we might want to know how many distinct values appear in a variable:\n\nstocks %&gt;% summarise(unique = n_distinct(SPSTOCK_D))\n\n  unique\n1      2\n\n\nWe can also check how many rows are fully complete—meaning no missing values in any column—using complete.cases():\n\nstocks %&gt;% summarise(complete_rows = sum(complete.cases(.)))\n\n  complete_rows\n1            82\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data with tidyverse",
      "Counting and Missing Values"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/summary-function.html",
    "href": "tutorials/summarizing-data-baseR/summary-function.html",
    "title": "The summary function",
    "section": "",
    "text": "We can produce several of these summary statistics at once by using the summary() command from ‘base R’ (i.e. without installing packages). You can use this on your entire dataset at once.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the summary statistics for the entire stocks dataset.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nsummary(stocks)\n\n      YEAR          TBONDS           SPSTOCK          TBONDS_D     \n Min.   :1928   Min.   :-11.000   Min.   :-44.00   Min.   :0.0000  \n 1st Qu.:1950   1st Qu.:  1.000   1st Qu.: -1.00   1st Qu.:1.0000  \n Median :1972   Median :  4.000   Median : 14.00   Median :1.0000  \n Mean   :1972   Mean   :  5.463   Mean   : 11.42   Mean   :0.8049  \n 3rd Qu.:1993   3rd Qu.:  9.000   3rd Qu.: 25.25   3rd Qu.:1.0000  \n Max.   :2015   Max.   : 33.000   Max.   : 53.00   Max.   :1.0000  \n                NA's   :6                          NA's   :6       \n   SPSTOCK_D     \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :1.0000  \n Mean   :0.7159  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n                 \n\n\n\n\n\n\n\nIf you want to retrieve descriptive statistics for multiple variables, you can use the entire data frame as an argument for summary(). You can also select a subset of variables (columns) by using, for example, stocks[1:3] as an argument to select the first three columns or stocks[c(1,2,4)] to select column 1, 2 and 4. Try it out!\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the summary statistics for column 2 and 5 of stocks.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nsummary(stocks[c(1,5)])\n\n      YEAR        SPSTOCK_D     \n Min.   :1928   Min.   :0.0000  \n 1st Qu.:1950   1st Qu.:0.0000  \n Median :1972   Median :1.0000  \n Mean   :1972   Mean   :0.7159  \n 3rd Qu.:1993   3rd Qu.:1.0000  \n Max.   :2015   Max.   :1.0000  \n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data",
      "Summary function"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/index.html",
    "href": "tutorials/summarizing-data-baseR/index.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "In this tutorial you will learn how to use R to calculate descriptive statistics such as the mean, mode, median, standard deviation, and range.\nWe assume that you already know the following:\n\nWhat measures of central tendency are and when to use them\nWhat measures of dispersion are and when to use them\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/mean-min-max.html",
    "href": "tutorials/summarizing-data-baseR/mean-min-max.html",
    "title": "Calculating summary statistics of variables",
    "section": "",
    "text": "The examples in this lesson are based on a data set about profits from stocks and bonds. We can take a look at the data using head(), which shows us the first 6 rows of the data frame.\n\nhead(stocks)\n\n  YEAR TBONDS SPSTOCK TBONDS_D SPSTOCK_D\n1 1928      1      44        1         1\n2 1929      4      -8        1         0\n3 1930     NA     -25       NA         0\n4 1931     -3     -44        0         0\n5 1932      9      -9        1         0\n6 1933      2      50        1         1\n\n\nThe data set contains five variables. For which we provide some extra information here:\n\nYEAR: the year\nTBONDS: Price of Treasury Bonds, US government bonds. For example, a figure of 4 means an increase of 4%.\nSPSTOCKS: S&P 500 Stocks price, shares of the 500 largest US companies. For example, a figure of 44 means an increase of 44%.\nTBONDS_D: Indicates whether Treasury Bonds rose (1) or fell/remained the same (0) that year.\nSPSTOCKS_D: Indicates whether the S&P 500 stocks rose (1) or fell/remained the same (0) that year.\n\nThe str() function tells us what the class of each variable is. You will notice that most variables in the stocks data contain integers.\n\nstr(stocks)\n\n'data.frame':   88 obs. of  5 variables:\n $ YEAR     : int  1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 ...\n $ TBONDS   : int  1 4 NA -3 9 2 8 4 5 1 ...\n $ SPSTOCK  : int  44 -8 -25 -44 -9 50 -1 47 32 -35 ...\n $ TBONDS_D : int  1 1 NA 0 1 1 1 1 1 1 ...\n $ SPSTOCK_D: int  1 0 0 0 0 1 0 1 1 0 ...\n\n\nWe will explore this data set further. At the start, we are interested to get more information about our collected variables. In most cases, we want at least to know the following parameters:\n\nthe mean\nthe standard deviation\nthe range (minimum and maximum)\n\nFor this purpose, R provides the following functions:\n\nmean()\nsd()\nmin()\nmax()\n\nYou can use these for a single variable (at a time). You can call up each variable by adding $‘variable name’ after the name of the data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the mean of SPSTOCKS from the stocks dataset.\nNOTE: The songs dataset is already loaded in the working directory of this webr session.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nmean(stocks$SPSTOCKS)\n\n[1] NA\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data",
      "Mean, minimum and maximum"
    ]
  },
  {
    "objectID": "tutorials/pivot/wide_vs_long.html",
    "href": "tutorials/pivot/wide_vs_long.html",
    "title": "Long Format vs. Wide Format",
    "section": "",
    "text": "Choosing between long and wide format depends on what you want to do with your data. Long format is usually better for analysis and visualization, while wide format is often better for inspection and for methods that require matrix-like inputs. Being able to move between the two lets you work in the most convenient form for each task.",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "Wide Format vs Long Format"
    ]
  },
  {
    "objectID": "tutorials/pivot/wide_vs_long.html#long-format",
    "href": "tutorials/pivot/wide_vs_long.html#long-format",
    "title": "Long Format vs. Wide Format",
    "section": "Long Format",
    "text": "Long Format\nLong format works well when you want to summarize or visualize repeated measures. Once each item becomes a row category, grouping and plotting become much simpler.\n\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"bfi\", package = \"psych\")\nbfi_long &lt;- bfi %&gt;%\n  pivot_longer(A1:A5, names_to = \"item\", values_to = \"response\") %&gt;%\n  select(item, response, everything())\nhead(bfi_long)\n\n# A tibble: 6 × 25\n  item  response    C1    C2    C3    C4    C5    E1    E2    E3    E4    E5\n  &lt;chr&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 A1           2     2     3     3     4     4     3     3     3     4     4\n2 A2           4     2     3     3     4     4     3     3     3     4     4\n3 A3           3     2     3     3     4     4     3     3     3     4     4\n4 A4           4     2     3     3     4     4     3     3     3     4     4\n5 A5           4     2     3     3     4     4     3     3     3     4     4\n6 A1           2     5     4     4     3     4     1     1     6     4     3\n# ℹ 13 more variables: N1 &lt;int&gt;, N2 &lt;int&gt;, N3 &lt;int&gt;, N4 &lt;int&gt;, N5 &lt;int&gt;,\n#   O1 &lt;int&gt;, O2 &lt;int&gt;, O3 &lt;int&gt;, O4 &lt;int&gt;, O5 &lt;int&gt;, gender &lt;int&gt;,\n#   education &lt;int&gt;, age &lt;int&gt;\n\nbfi_long %&gt;%\n  group_by(item) %&gt;%\n  summarise(mean = mean(response, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  item   mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 A1     2.41\n2 A2     4.80\n3 A3     4.60\n4 A4     4.70\n5 A5     4.56\n\n\nLong format also works naturally with ggplot2, especially for faceting.",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "Wide Format vs Long Format"
    ]
  },
  {
    "objectID": "tutorials/pivot/wide_vs_long.html#wide-format",
    "href": "tutorials/pivot/wide_vs_long.html#wide-format",
    "title": "Long Format vs. Wide Format",
    "section": "Wide Format",
    "text": "Wide Format\nWide format is helpful when you need all variables as separate columns, such as for correlation matrices or for exporting readable summary tables.\n\nbfi %&gt;%\n  select(A1:A5) %&gt;%\n  cor(use = \"pairwise\")\n\n           A1         A2         A3         A4         A5\nA1  1.0000000 -0.3401932 -0.2652471 -0.1464245 -0.1814383\nA2 -0.3401932  1.0000000  0.4850980  0.3350872  0.3900836\nA3 -0.2652471  0.4850980  1.0000000  0.3604283  0.5041411\nA4 -0.1464245  0.3350872  0.3604283  1.0000000  0.3075373\nA5 -0.1814383  0.3900836  0.5041411  0.3075373  1.0000000\n\n\nYou can also pivot back to wide format after summarizing:\n\nbfi_long %&gt;%\n  group_by(item) %&gt;%\n  summarise(mean = mean(response, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = item, values_from = mean)\n\n# A tibble: 1 × 5\n     A1    A2    A3    A4    A5\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2.41  4.80  4.60  4.70  4.56",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "Wide Format vs Long Format"
    ]
  },
  {
    "objectID": "tutorials/pivot/pivot_wider.html",
    "href": "tutorials/pivot/pivot_wider.html",
    "title": "From Long to Wide",
    "section": "",
    "text": "Sometimes we need to reverse the transformation. Imagine we start from a long dataset like the one above and want to restore the original columns—this time using pivot_wider():\nlibrary(dplyr)\nlibrary(tidyr)\nbfi_small_wide &lt;- bfi_small_long %&gt;%\n  pivot_wider(\n    names_from = item,\n    values_from = response\n  )\nThe resulting dataset has one row per participant (id) and separate columns for A1, A2, and A3 again. pivot_wider() is especially useful when creating summary tables or preparing data for export.",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "From Long to Wide"
    ]
  },
  {
    "objectID": "tutorials/pivot/pivot_wider.html#dealing-with-missing-values",
    "href": "tutorials/pivot/pivot_wider.html#dealing-with-missing-values",
    "title": "From Long to Wide",
    "section": "Dealing with Missing Values",
    "text": "Dealing with Missing Values\nPivoting can introduce missing values when not every subject has measurements in every category. pivot_wider() gives you a simple way to fill missing cells with a chosen value:\n\nbfi_small_long %&gt;%\n  pivot_wider(\n    names_from = item,\n    values_from = response,\n    values_fill = 0     # or NA, or any other placeholder\n  )\n\n# A tibble: 2,800 × 4\n      id    A1    A2    A3\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     1     2     4     3\n 2     2     2     4     5\n 3     3     5     4     5\n 4     4     4     4     6\n 5     5     2     3     3\n 6     6     6     6     5\n 7     7     2     5     5\n 8     8     4     3     1\n 9     9     4     3     6\n10    10     2     5     6\n# ℹ 2,790 more rows\n\n\nThis is useful for preparing matrices for models or visualizations that require complete grids.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTransform the this long dataset back to wide format using pivot_wider(), ensuring that missing values are filled with -1.\n\n\n# A tibble: 6 × 3\n     id variable value\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 X           10\n2     1 Y           20\n3     2 X           30\n4     3 X           40\n5     3 Y           50\n6     3 Z           60\n\n\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nlong_data %&gt;%\n  pivot_wider(\n    names_from = variable,\n    values_from = value,\n    values_fill = -1\n  )\n\n# A tibble: 3 × 4\n     id     X     Y     Z\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10    20    -1\n2     2    30    -1    -1\n3     3    40    50    60",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "From Long to Wide"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/crosstable.html",
    "href": "tutorials/frequency-tables-tidy/crosstable.html",
    "title": "Crosstables",
    "section": "",
    "text": "To create a cross-tabulation (a two-way table), tidyverse uses count() together with multiple variables:\n\nlibrary(dplyr)\nsongs %&gt;%\n  count(THEME, YEAR)\n\n                   THEME YEAR  n\n1             Heartbreak 1931  1\n2             Heartbreak 1951  1\n3             Heartbreak 1958  2\n4             Heartbreak 1959  1\n5             Heartbreak 1961  2\n6             Heartbreak 1962  2\n7             Heartbreak 1963  2\n8             Heartbreak 1964  6\n9             Heartbreak 1965  4\n10            Heartbreak 1966  9\n11            Heartbreak 1967  3\n12            Heartbreak 1968  4\n13            Heartbreak 1969  4\n14            Heartbreak 1970  5\n15            Heartbreak 1971  5\n16            Heartbreak 1972  5\n17            Heartbreak 1973  3\n18            Heartbreak 1974  2\n19            Heartbreak 1975  5\n20            Heartbreak 1976  1\n21            Heartbreak 1977  2\n22            Heartbreak 1978  5\n23            Heartbreak 1979  4\n24            Heartbreak 1980  3\n25            Heartbreak 1981  2\n26            Heartbreak 1982  1\n27            Heartbreak 1983  3\n28            Heartbreak 1984  4\n29            Heartbreak 1985  1\n30            Heartbreak 1986  5\n31            Heartbreak 1987  2\n32            Heartbreak 1988  2\n33            Heartbreak 1989  1\n34            Heartbreak 1990  2\n35            Heartbreak 1991  1\n36            Heartbreak 1992  1\n37            Heartbreak 1993  1\n38            Heartbreak 1994  2\n39            Heartbreak 1995  3\n40            Heartbreak 1996  2\n41            Heartbreak 1997  3\n42            Heartbreak 1998  3\n43            Heartbreak 1999  2\n44            Heartbreak 2000  2\n45            Heartbreak 2001  1\n46            Heartbreak 2002  1\n47            Heartbreak 2003  4\n48            Heartbreak 2004  4\n49            Heartbreak 2005  3\n50            Heartbreak 2006  2\n51            Heartbreak 2007  3\n52            Heartbreak 2008  3\n53        Life_and_death 1928  2\n54        Life_and_death 1935  1\n55        Life_and_death 1941  1\n56        Life_and_death 1949  1\n57        Life_and_death 1950  1\n58        Life_and_death 1957  1\n59        Life_and_death 1958  2\n60        Life_and_death 1959  3\n61        Life_and_death 1961  2\n62        Life_and_death 1962  1\n63        Life_and_death 1963  2\n64        Life_and_death 1964  2\n65        Life_and_death 1965  4\n66        Life_and_death 1966  5\n67        Life_and_death 1967  2\n68        Life_and_death 1968  4\n69        Life_and_death 1969  4\n70        Life_and_death 1970  4\n71        Life_and_death 1971  5\n72        Life_and_death 1972  6\n73        Life_and_death 1973  5\n74        Life_and_death 1974  5\n75        Life_and_death 1975  1\n76        Life_and_death 1976  1\n77        Life_and_death 1977  2\n78        Life_and_death 1978  2\n79        Life_and_death 1979  2\n80        Life_and_death 1980  3\n81        Life_and_death 1981  1\n82        Life_and_death 1982  3\n83        Life_and_death 1983  1\n84        Life_and_death 1984  2\n85        Life_and_death 1985  1\n86        Life_and_death 1986  1\n87        Life_and_death 1987  2\n88        Life_and_death 1988  3\n89        Life_and_death 1989  2\n90        Life_and_death 1990  2\n91        Life_and_death 1991  4\n92        Life_and_death 1992  2\n93        Life_and_death 1993  2\n94        Life_and_death 1994  8\n95        Life_and_death 1996  2\n96        Life_and_death 1997  3\n97        Life_and_death 2000  4\n98        Life_and_death 2002  1\n99        Life_and_death 2003  2\n100       Life_and_death 2004  1\n101       Life_and_death 2005  3\n102       Life_and_death 2006  3\n103       Life_and_death 2007  2\n104       Life_and_death 2008  2\n105                 Love 1928  1\n106                 Love 1939  1\n107                 Love 1952  1\n108                 Love 1954  1\n109                 Love 1955  1\n110                 Love 1956  5\n111                 Love 1958  1\n112                 Love 1959  2\n113                 Love 1960  1\n114                 Love 1961  4\n115                 Love 1962  2\n116                 Love 1963  5\n117                 Love 1964 11\n118                 Love 1965  4\n119                 Love 1966 11\n120                 Love 1967  6\n121                 Love 1968  7\n122                 Love 1969  2\n123                 Love 1970  3\n124                 Love 1971  3\n125                 Love 1972  6\n126                 Love 1973  8\n127                 Love 1974  6\n128                 Love 1975  3\n129                 Love 1976  1\n130                 Love 1977  5\n131                 Love 1978  4\n132                 Love 1979  3\n133                 Love 1982  2\n134                 Love 1984  1\n135                 Love 1985  2\n136                 Love 1986  2\n137                 Love 1987  2\n138                 Love 1988  1\n139                 Love 1989  2\n140                 Love 1992  2\n141                 Love 1993  1\n142                 Love 1994  3\n143                 Love 1995  3\n144                 Love 1996  1\n145                 Love 2000  2\n146                 Love 2001  1\n147                 Love 2003  2\n148                 Love 2007  3\n149                 Love 2008  1\n150          Party_songs 1938  1\n151          Party_songs 1944  1\n152          Party_songs 1959  1\n153          Party_songs 1962  1\n154          Party_songs 1963  1\n155          Party_songs 1965  3\n156          Party_songs 1967  5\n157          Party_songs 1968  5\n158          Party_songs 1970  1\n159          Party_songs 1972  4\n160          Party_songs 1973  2\n161          Party_songs 1974  6\n162          Party_songs 1975  4\n163          Party_songs 1976  9\n164          Party_songs 1977  6\n165          Party_songs 1978  6\n166          Party_songs 1979 17\n167          Party_songs 1980  5\n168          Party_songs 1981  7\n169          Party_songs 1982  5\n170          Party_songs 1983 11\n171          Party_songs 1984  7\n172          Party_songs 1985  2\n173          Party_songs 1986  1\n174          Party_songs 1987  4\n175          Party_songs 1988  1\n176          Party_songs 1989  4\n177          Party_songs 1990  2\n178          Party_songs 1991  1\n179          Party_songs 1992  2\n180          Party_songs 1993  1\n181          Party_songs 1994  1\n182          Party_songs 1995  3\n183          Party_songs 1997  1\n184          Party_songs 1998  1\n185          Party_songs 1999  6\n186          Party_songs 2000  1\n187          Party_songs 2001  3\n188          Party_songs 2002  4\n189          Party_songs 2003  2\n190          Party_songs 2004  2\n191          Party_songs 2005  3\n192          Party_songs 2006  1\n193          Party_songs 2007  2\n194          Party_songs 2008  6\n195    People_and_places 1916  1\n196    People_and_places 1922  1\n197    People_and_places 1928  2\n198    People_and_places 1931  1\n199    People_and_places 1932  1\n200    People_and_places 1936  1\n201    People_and_places 1939  1\n202    People_and_places 1940  1\n203    People_and_places 1941  1\n204    People_and_places 1946  2\n205    People_and_places 1950  1\n206    People_and_places 1951  2\n207    People_and_places 1953  1\n208    People_and_places 1954  2\n209    People_and_places 1955  1\n210    People_and_places 1956  4\n211    People_and_places 1958  1\n212    People_and_places 1959  3\n213    People_and_places 1960  3\n214    People_and_places 1961  5\n215    People_and_places 1962  1\n216    People_and_places 1963  3\n217    People_and_places 1964  3\n218    People_and_places 1965  8\n219    People_and_places 1966  7\n220    People_and_places 1967  9\n221    People_and_places 1968  8\n222    People_and_places 1969  5\n223    People_and_places 1971  6\n224    People_and_places 1972  2\n225    People_and_places 1973  2\n226    People_and_places 1974  4\n227    People_and_places 1975  4\n228    People_and_places 1976  1\n229    People_and_places 1977  4\n230    People_and_places 1978  2\n231    People_and_places 1979  3\n232    People_and_places 1980  5\n233    People_and_places 1981  2\n234    People_and_places 1983  5\n235    People_and_places 1984  2\n236    People_and_places 1985  3\n237    People_and_places 1986  2\n238    People_and_places 1987  2\n239    People_and_places 1988  1\n240    People_and_places 1989  1\n241    People_and_places 1991  1\n242    People_and_places 1993  2\n243    People_and_places 1995  1\n244    People_and_places 1998  1\n245    People_and_places 1999  1\n246    People_and_places 2001  1\n247    People_and_places 2002  1\n248    People_and_places 2005  3\n249    People_and_places 2006  2\n250    People_and_places 2007  1\n251    People_and_places 2008  1\n252 Politics_and_protest 1929  2\n253 Politics_and_protest 1932  1\n254 Politics_and_protest 1938  1\n255 Politics_and_protest 1939  1\n256 Politics_and_protest 1944  1\n257 Politics_and_protest 1960  1\n258 Politics_and_protest 1963  3\n259 Politics_and_protest 1964  5\n260 Politics_and_protest 1965  6\n261 Politics_and_protest 1966  1\n262 Politics_and_protest 1967  3\n263 Politics_and_protest 1968  6\n264 Politics_and_protest 1969  8\n265 Politics_and_protest 1970  8\n266 Politics_and_protest 1971  7\n267 Politics_and_protest 1973  2\n268 Politics_and_protest 1974  1\n269 Politics_and_protest 1975  2\n270 Politics_and_protest 1976  4\n271 Politics_and_protest 1977  6\n272 Politics_and_protest 1978  2\n273 Politics_and_protest 1979  4\n274 Politics_and_protest 1980  6\n275 Politics_and_protest 1981  2\n276 Politics_and_protest 1982  4\n277 Politics_and_protest 1983  1\n278 Politics_and_protest 1984  4\n279 Politics_and_protest 1985  7\n280 Politics_and_protest 1986  2\n281 Politics_and_protest 1987  2\n282 Politics_and_protest 1988  4\n283 Politics_and_protest 1989  5\n284 Politics_and_protest 1990  1\n285 Politics_and_protest 1991  1\n286 Politics_and_protest 1992  4\n287 Politics_and_protest 1993  1\n288 Politics_and_protest 1996  1\n289 Politics_and_protest 1999  1\n290 Politics_and_protest 2000  1\n291 Politics_and_protest 2002  2\n292 Politics_and_protest 2003  1\n293 Politics_and_protest 2004  2\n294 Politics_and_protest 2005  3\n295 Politics_and_protest 2006  3\n296 Politics_and_protest 2007  4\n297 Politics_and_protest 2008  4\n298                  Sex 1928  1\n299                  Sex 1954  1\n300                  Sex 1955  1\n301                  Sex 1957  2\n302                  Sex 1961  1\n303                  Sex 1963  1\n304                  Sex 1965  4\n305                  Sex 1966  4\n306                  Sex 1967  5\n307                  Sex 1968  5\n308                  Sex 1969  8\n309                  Sex 1970  2\n310                  Sex 1971  4\n311                  Sex 1972  4\n312                  Sex 1973  2\n313                  Sex 1974  3\n314                  Sex 1975  4\n315                  Sex 1976  1\n316                  Sex 1977  2\n317                  Sex 1978  4\n318                  Sex 1979  1\n319                  Sex 1980  3\n320                  Sex 1981  5\n321                  Sex 1982  3\n322                  Sex 1983  4\n323                  Sex 1984  2\n324                  Sex 1985  2\n325                  Sex 1986  4\n326                  Sex 1987  4\n327                  Sex 1988  3\n328                  Sex 1989  4\n329                  Sex 1990  3\n330                  Sex 1991  2\n331                  Sex 1992  3\n332                  Sex 1993  3\n333                  Sex 1994  4\n334                  Sex 1995  2\n335                  Sex 1996  1\n336                  Sex 1997  1\n337                  Sex 1998  2\n338                  Sex 1999  1\n339                  Sex 2000  2\n340                  Sex 2001  1\n341                  Sex 2002  3\n342                  Sex 2003  3\n343                  Sex 2006  3\n344                  Sex 2007  1\n345                  Sex 2008  2\n346                 &lt;NA&gt;   NA  6\n\n\nThis produces a long-format contingency table, which is often easier to manipulate than the wide matrix returned by table().\nIf you want the joint proportions:\n\nsongs %&gt;%\n  count(THEME, YEAR) %&gt;%\n  mutate(prop = n / sum(n))\n\n                   THEME YEAR  n  prop\n1             Heartbreak 1931  1 0.001\n2             Heartbreak 1951  1 0.001\n3             Heartbreak 1958  2 0.002\n4             Heartbreak 1959  1 0.001\n5             Heartbreak 1961  2 0.002\n6             Heartbreak 1962  2 0.002\n7             Heartbreak 1963  2 0.002\n8             Heartbreak 1964  6 0.006\n9             Heartbreak 1965  4 0.004\n10            Heartbreak 1966  9 0.009\n11            Heartbreak 1967  3 0.003\n12            Heartbreak 1968  4 0.004\n13            Heartbreak 1969  4 0.004\n14            Heartbreak 1970  5 0.005\n15            Heartbreak 1971  5 0.005\n16            Heartbreak 1972  5 0.005\n17            Heartbreak 1973  3 0.003\n18            Heartbreak 1974  2 0.002\n19            Heartbreak 1975  5 0.005\n20            Heartbreak 1976  1 0.001\n21            Heartbreak 1977  2 0.002\n22            Heartbreak 1978  5 0.005\n23            Heartbreak 1979  4 0.004\n24            Heartbreak 1980  3 0.003\n25            Heartbreak 1981  2 0.002\n26            Heartbreak 1982  1 0.001\n27            Heartbreak 1983  3 0.003\n28            Heartbreak 1984  4 0.004\n29            Heartbreak 1985  1 0.001\n30            Heartbreak 1986  5 0.005\n31            Heartbreak 1987  2 0.002\n32            Heartbreak 1988  2 0.002\n33            Heartbreak 1989  1 0.001\n34            Heartbreak 1990  2 0.002\n35            Heartbreak 1991  1 0.001\n36            Heartbreak 1992  1 0.001\n37            Heartbreak 1993  1 0.001\n38            Heartbreak 1994  2 0.002\n39            Heartbreak 1995  3 0.003\n40            Heartbreak 1996  2 0.002\n41            Heartbreak 1997  3 0.003\n42            Heartbreak 1998  3 0.003\n43            Heartbreak 1999  2 0.002\n44            Heartbreak 2000  2 0.002\n45            Heartbreak 2001  1 0.001\n46            Heartbreak 2002  1 0.001\n47            Heartbreak 2003  4 0.004\n48            Heartbreak 2004  4 0.004\n49            Heartbreak 2005  3 0.003\n50            Heartbreak 2006  2 0.002\n51            Heartbreak 2007  3 0.003\n52            Heartbreak 2008  3 0.003\n53        Life_and_death 1928  2 0.002\n54        Life_and_death 1935  1 0.001\n55        Life_and_death 1941  1 0.001\n56        Life_and_death 1949  1 0.001\n57        Life_and_death 1950  1 0.001\n58        Life_and_death 1957  1 0.001\n59        Life_and_death 1958  2 0.002\n60        Life_and_death 1959  3 0.003\n61        Life_and_death 1961  2 0.002\n62        Life_and_death 1962  1 0.001\n63        Life_and_death 1963  2 0.002\n64        Life_and_death 1964  2 0.002\n65        Life_and_death 1965  4 0.004\n66        Life_and_death 1966  5 0.005\n67        Life_and_death 1967  2 0.002\n68        Life_and_death 1968  4 0.004\n69        Life_and_death 1969  4 0.004\n70        Life_and_death 1970  4 0.004\n71        Life_and_death 1971  5 0.005\n72        Life_and_death 1972  6 0.006\n73        Life_and_death 1973  5 0.005\n74        Life_and_death 1974  5 0.005\n75        Life_and_death 1975  1 0.001\n76        Life_and_death 1976  1 0.001\n77        Life_and_death 1977  2 0.002\n78        Life_and_death 1978  2 0.002\n79        Life_and_death 1979  2 0.002\n80        Life_and_death 1980  3 0.003\n81        Life_and_death 1981  1 0.001\n82        Life_and_death 1982  3 0.003\n83        Life_and_death 1983  1 0.001\n84        Life_and_death 1984  2 0.002\n85        Life_and_death 1985  1 0.001\n86        Life_and_death 1986  1 0.001\n87        Life_and_death 1987  2 0.002\n88        Life_and_death 1988  3 0.003\n89        Life_and_death 1989  2 0.002\n90        Life_and_death 1990  2 0.002\n91        Life_and_death 1991  4 0.004\n92        Life_and_death 1992  2 0.002\n93        Life_and_death 1993  2 0.002\n94        Life_and_death 1994  8 0.008\n95        Life_and_death 1996  2 0.002\n96        Life_and_death 1997  3 0.003\n97        Life_and_death 2000  4 0.004\n98        Life_and_death 2002  1 0.001\n99        Life_and_death 2003  2 0.002\n100       Life_and_death 2004  1 0.001\n101       Life_and_death 2005  3 0.003\n102       Life_and_death 2006  3 0.003\n103       Life_and_death 2007  2 0.002\n104       Life_and_death 2008  2 0.002\n105                 Love 1928  1 0.001\n106                 Love 1939  1 0.001\n107                 Love 1952  1 0.001\n108                 Love 1954  1 0.001\n109                 Love 1955  1 0.001\n110                 Love 1956  5 0.005\n111                 Love 1958  1 0.001\n112                 Love 1959  2 0.002\n113                 Love 1960  1 0.001\n114                 Love 1961  4 0.004\n115                 Love 1962  2 0.002\n116                 Love 1963  5 0.005\n117                 Love 1964 11 0.011\n118                 Love 1965  4 0.004\n119                 Love 1966 11 0.011\n120                 Love 1967  6 0.006\n121                 Love 1968  7 0.007\n122                 Love 1969  2 0.002\n123                 Love 1970  3 0.003\n124                 Love 1971  3 0.003\n125                 Love 1972  6 0.006\n126                 Love 1973  8 0.008\n127                 Love 1974  6 0.006\n128                 Love 1975  3 0.003\n129                 Love 1976  1 0.001\n130                 Love 1977  5 0.005\n131                 Love 1978  4 0.004\n132                 Love 1979  3 0.003\n133                 Love 1982  2 0.002\n134                 Love 1984  1 0.001\n135                 Love 1985  2 0.002\n136                 Love 1986  2 0.002\n137                 Love 1987  2 0.002\n138                 Love 1988  1 0.001\n139                 Love 1989  2 0.002\n140                 Love 1992  2 0.002\n141                 Love 1993  1 0.001\n142                 Love 1994  3 0.003\n143                 Love 1995  3 0.003\n144                 Love 1996  1 0.001\n145                 Love 2000  2 0.002\n146                 Love 2001  1 0.001\n147                 Love 2003  2 0.002\n148                 Love 2007  3 0.003\n149                 Love 2008  1 0.001\n150          Party_songs 1938  1 0.001\n151          Party_songs 1944  1 0.001\n152          Party_songs 1959  1 0.001\n153          Party_songs 1962  1 0.001\n154          Party_songs 1963  1 0.001\n155          Party_songs 1965  3 0.003\n156          Party_songs 1967  5 0.005\n157          Party_songs 1968  5 0.005\n158          Party_songs 1970  1 0.001\n159          Party_songs 1972  4 0.004\n160          Party_songs 1973  2 0.002\n161          Party_songs 1974  6 0.006\n162          Party_songs 1975  4 0.004\n163          Party_songs 1976  9 0.009\n164          Party_songs 1977  6 0.006\n165          Party_songs 1978  6 0.006\n166          Party_songs 1979 17 0.017\n167          Party_songs 1980  5 0.005\n168          Party_songs 1981  7 0.007\n169          Party_songs 1982  5 0.005\n170          Party_songs 1983 11 0.011\n171          Party_songs 1984  7 0.007\n172          Party_songs 1985  2 0.002\n173          Party_songs 1986  1 0.001\n174          Party_songs 1987  4 0.004\n175          Party_songs 1988  1 0.001\n176          Party_songs 1989  4 0.004\n177          Party_songs 1990  2 0.002\n178          Party_songs 1991  1 0.001\n179          Party_songs 1992  2 0.002\n180          Party_songs 1993  1 0.001\n181          Party_songs 1994  1 0.001\n182          Party_songs 1995  3 0.003\n183          Party_songs 1997  1 0.001\n184          Party_songs 1998  1 0.001\n185          Party_songs 1999  6 0.006\n186          Party_songs 2000  1 0.001\n187          Party_songs 2001  3 0.003\n188          Party_songs 2002  4 0.004\n189          Party_songs 2003  2 0.002\n190          Party_songs 2004  2 0.002\n191          Party_songs 2005  3 0.003\n192          Party_songs 2006  1 0.001\n193          Party_songs 2007  2 0.002\n194          Party_songs 2008  6 0.006\n195    People_and_places 1916  1 0.001\n196    People_and_places 1922  1 0.001\n197    People_and_places 1928  2 0.002\n198    People_and_places 1931  1 0.001\n199    People_and_places 1932  1 0.001\n200    People_and_places 1936  1 0.001\n201    People_and_places 1939  1 0.001\n202    People_and_places 1940  1 0.001\n203    People_and_places 1941  1 0.001\n204    People_and_places 1946  2 0.002\n205    People_and_places 1950  1 0.001\n206    People_and_places 1951  2 0.002\n207    People_and_places 1953  1 0.001\n208    People_and_places 1954  2 0.002\n209    People_and_places 1955  1 0.001\n210    People_and_places 1956  4 0.004\n211    People_and_places 1958  1 0.001\n212    People_and_places 1959  3 0.003\n213    People_and_places 1960  3 0.003\n214    People_and_places 1961  5 0.005\n215    People_and_places 1962  1 0.001\n216    People_and_places 1963  3 0.003\n217    People_and_places 1964  3 0.003\n218    People_and_places 1965  8 0.008\n219    People_and_places 1966  7 0.007\n220    People_and_places 1967  9 0.009\n221    People_and_places 1968  8 0.008\n222    People_and_places 1969  5 0.005\n223    People_and_places 1971  6 0.006\n224    People_and_places 1972  2 0.002\n225    People_and_places 1973  2 0.002\n226    People_and_places 1974  4 0.004\n227    People_and_places 1975  4 0.004\n228    People_and_places 1976  1 0.001\n229    People_and_places 1977  4 0.004\n230    People_and_places 1978  2 0.002\n231    People_and_places 1979  3 0.003\n232    People_and_places 1980  5 0.005\n233    People_and_places 1981  2 0.002\n234    People_and_places 1983  5 0.005\n235    People_and_places 1984  2 0.002\n236    People_and_places 1985  3 0.003\n237    People_and_places 1986  2 0.002\n238    People_and_places 1987  2 0.002\n239    People_and_places 1988  1 0.001\n240    People_and_places 1989  1 0.001\n241    People_and_places 1991  1 0.001\n242    People_and_places 1993  2 0.002\n243    People_and_places 1995  1 0.001\n244    People_and_places 1998  1 0.001\n245    People_and_places 1999  1 0.001\n246    People_and_places 2001  1 0.001\n247    People_and_places 2002  1 0.001\n248    People_and_places 2005  3 0.003\n249    People_and_places 2006  2 0.002\n250    People_and_places 2007  1 0.001\n251    People_and_places 2008  1 0.001\n252 Politics_and_protest 1929  2 0.002\n253 Politics_and_protest 1932  1 0.001\n254 Politics_and_protest 1938  1 0.001\n255 Politics_and_protest 1939  1 0.001\n256 Politics_and_protest 1944  1 0.001\n257 Politics_and_protest 1960  1 0.001\n258 Politics_and_protest 1963  3 0.003\n259 Politics_and_protest 1964  5 0.005\n260 Politics_and_protest 1965  6 0.006\n261 Politics_and_protest 1966  1 0.001\n262 Politics_and_protest 1967  3 0.003\n263 Politics_and_protest 1968  6 0.006\n264 Politics_and_protest 1969  8 0.008\n265 Politics_and_protest 1970  8 0.008\n266 Politics_and_protest 1971  7 0.007\n267 Politics_and_protest 1973  2 0.002\n268 Politics_and_protest 1974  1 0.001\n269 Politics_and_protest 1975  2 0.002\n270 Politics_and_protest 1976  4 0.004\n271 Politics_and_protest 1977  6 0.006\n272 Politics_and_protest 1978  2 0.002\n273 Politics_and_protest 1979  4 0.004\n274 Politics_and_protest 1980  6 0.006\n275 Politics_and_protest 1981  2 0.002\n276 Politics_and_protest 1982  4 0.004\n277 Politics_and_protest 1983  1 0.001\n278 Politics_and_protest 1984  4 0.004\n279 Politics_and_protest 1985  7 0.007\n280 Politics_and_protest 1986  2 0.002\n281 Politics_and_protest 1987  2 0.002\n282 Politics_and_protest 1988  4 0.004\n283 Politics_and_protest 1989  5 0.005\n284 Politics_and_protest 1990  1 0.001\n285 Politics_and_protest 1991  1 0.001\n286 Politics_and_protest 1992  4 0.004\n287 Politics_and_protest 1993  1 0.001\n288 Politics_and_protest 1996  1 0.001\n289 Politics_and_protest 1999  1 0.001\n290 Politics_and_protest 2000  1 0.001\n291 Politics_and_protest 2002  2 0.002\n292 Politics_and_protest 2003  1 0.001\n293 Politics_and_protest 2004  2 0.002\n294 Politics_and_protest 2005  3 0.003\n295 Politics_and_protest 2006  3 0.003\n296 Politics_and_protest 2007  4 0.004\n297 Politics_and_protest 2008  4 0.004\n298                  Sex 1928  1 0.001\n299                  Sex 1954  1 0.001\n300                  Sex 1955  1 0.001\n301                  Sex 1957  2 0.002\n302                  Sex 1961  1 0.001\n303                  Sex 1963  1 0.001\n304                  Sex 1965  4 0.004\n305                  Sex 1966  4 0.004\n306                  Sex 1967  5 0.005\n307                  Sex 1968  5 0.005\n308                  Sex 1969  8 0.008\n309                  Sex 1970  2 0.002\n310                  Sex 1971  4 0.004\n311                  Sex 1972  4 0.004\n312                  Sex 1973  2 0.002\n313                  Sex 1974  3 0.003\n314                  Sex 1975  4 0.004\n315                  Sex 1976  1 0.001\n316                  Sex 1977  2 0.002\n317                  Sex 1978  4 0.004\n318                  Sex 1979  1 0.001\n319                  Sex 1980  3 0.003\n320                  Sex 1981  5 0.005\n321                  Sex 1982  3 0.003\n322                  Sex 1983  4 0.004\n323                  Sex 1984  2 0.002\n324                  Sex 1985  2 0.002\n325                  Sex 1986  4 0.004\n326                  Sex 1987  4 0.004\n327                  Sex 1988  3 0.003\n328                  Sex 1989  4 0.004\n329                  Sex 1990  3 0.003\n330                  Sex 1991  2 0.002\n331                  Sex 1992  3 0.003\n332                  Sex 1993  3 0.003\n333                  Sex 1994  4 0.004\n334                  Sex 1995  2 0.002\n335                  Sex 1996  1 0.001\n336                  Sex 1997  1 0.001\n337                  Sex 1998  2 0.002\n338                  Sex 1999  1 0.001\n339                  Sex 2000  2 0.002\n340                  Sex 2001  1 0.001\n341                  Sex 2002  3 0.003\n342                  Sex 2003  3 0.003\n343                  Sex 2006  3 0.003\n344                  Sex 2007  1 0.001\n345                  Sex 2008  2 0.002\n346                 &lt;NA&gt;   NA  6 0.006\n\n\nIf you want conditional proportions—for example, proportions within each theme—you can use group_by():\n\nsongs %&gt;%\n  count(THEME, YEAR) %&gt;%\n  group_by(THEME) %&gt;%\n  mutate(prop_within_theme = n / sum(n)) %&gt;%\n  ungroup()\n\n# A tibble: 346 × 4\n   THEME       YEAR     n prop_within_theme\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;             &lt;dbl&gt;\n 1 Heartbreak  1931     1           0.00690\n 2 Heartbreak  1951     1           0.00690\n 3 Heartbreak  1958     2           0.0138 \n 4 Heartbreak  1959     1           0.00690\n 5 Heartbreak  1961     2           0.0138 \n 6 Heartbreak  1962     2           0.0138 \n 7 Heartbreak  1963     2           0.0138 \n 8 Heartbreak  1964     6           0.0414 \n 9 Heartbreak  1965     4           0.0276 \n10 Heartbreak  1966     9           0.0621 \n# ℹ 336 more rows\n\n\nThis is the tidyverse equivalent of:\n\nprop.table(table(sex, education), 1) (row proportions)\nprop.table(table(sex, education), 2) (column proportions)\n\nbut expressed through clear piping grammar.\nIf needed, you can pivot from long format to wide format:\n\nlibrary(tidyr)\nsongs %&gt;%\n  count(THEME, YEAR) %&gt;%\n  tidyr::pivot_wider(names_from = YEAR, values_from = n, values_fill = 0)\n\n# A tibble: 8 × 76\n  THEME    `1931` `1951` `1958` `1959` `1961` `1962` `1963` `1964` `1965` `1966`\n  &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 Heartbr…      1      1      2      1      2      2      2      6      4      9\n2 Life_an…      0      0      2      3      2      1      2      2      4      5\n3 Love          0      0      1      2      4      2      5     11      4     11\n4 Party_s…      0      0      0      1      0      1      1      0      3      0\n5 People_…      1      2      1      3      5      1      3      3      8      7\n6 Politic…      0      0      0      0      0      0      3      5      6      1\n7 Sex           0      0      0      0      1      0      1      0      4      4\n8 &lt;NA&gt;          0      0      0      0      0      0      0      0      0      0\n# ℹ 65 more variables: `1967` &lt;int&gt;, `1968` &lt;int&gt;, `1969` &lt;int&gt;, `1970` &lt;int&gt;,\n#   `1971` &lt;int&gt;, `1972` &lt;int&gt;, `1973` &lt;int&gt;, `1974` &lt;int&gt;, `1975` &lt;int&gt;,\n#   `1976` &lt;int&gt;, `1977` &lt;int&gt;, `1978` &lt;int&gt;, `1979` &lt;int&gt;, `1980` &lt;int&gt;,\n#   `1981` &lt;int&gt;, `1982` &lt;int&gt;, `1983` &lt;int&gt;, `1984` &lt;int&gt;, `1985` &lt;int&gt;,\n#   `1986` &lt;int&gt;, `1987` &lt;int&gt;, `1988` &lt;int&gt;, `1989` &lt;int&gt;, `1990` &lt;int&gt;,\n#   `1991` &lt;int&gt;, `1992` &lt;int&gt;, `1993` &lt;int&gt;, `1994` &lt;int&gt;, `1995` &lt;int&gt;,\n#   `1996` &lt;int&gt;, `1997` &lt;int&gt;, `1998` &lt;int&gt;, `1999` &lt;int&gt;, `2000` &lt;int&gt;, …\n\n\nThis produces the classic contingency matrix. In the next section we will talk more about pivoting data.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse",
      "Crosstables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/freq_tables.html",
    "href": "tutorials/frequency-tables-tidy/freq_tables.html",
    "title": "Frequency tables",
    "section": "",
    "text": "The tidyverse provides the count() function, a clean and expressive way to produce frequency tables. It takes a data frame and a variable, and returns a tibble with the variable’s values and their frequencies.\n\nlibrary(dplyr)\nsongs %&gt;% count(THEME)\n\n                 THEME   n\n1           Heartbreak 145\n2       Life_and_death 131\n3                 Love 139\n4          Party_songs 162\n5    People_and_places 145\n6 Politics_and_protest 141\n7                  Sex 131\n8                 &lt;NA&gt;   6\n\n\nThis gives a simple table with two columns:\n\nthe category (e.g., levels of education)\nthe number of cases in each category\n\nIf you want the categories sorted from most to least frequent, you can add:\n\nsongs %&gt;% count(THEME, sort = TRUE)\n\n                 THEME   n\n1          Party_songs 162\n2           Heartbreak 145\n3    People_and_places 145\n4 Politics_and_protest 141\n5                 Love 139\n6       Life_and_death 131\n7                  Sex 131\n8                 &lt;NA&gt;   6\n\n\nThis replaces sort(table(x)) with a single tidy and readable expression.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nMake a frequency table for the variable YEAR with dplyr.\nNOTE: The songs dataset and the dplyr package are already loaded in the working directory of this webr session.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nsongs %&gt;% count(YEAR)\n\n   YEAR  n\n1  1916  1\n2  1922  1\n3  1928  6\n4  1929  2\n5  1931  2\n6  1932  2\n7  1935  1\n8  1936  1\n9  1938  2\n10 1939  3\n11 1940  1\n12 1941  2\n13 1944  2\n14 1946  2\n15 1949  1\n16 1950  2\n17 1951  3\n18 1952  1\n19 1953  1\n20 1954  4\n21 1955  3\n22 1956  9\n23 1957  3\n24 1958  6\n25 1959 10\n26 1960  5\n27 1961 14\n28 1962  7\n29 1963 17\n30 1964 27\n31 1965 33\n32 1966 37\n33 1967 33\n34 1968 39\n35 1969 31\n36 1970 23\n37 1971 30\n38 1972 27\n39 1973 24\n40 1974 27\n41 1975 23\n42 1976 18\n43 1977 27\n44 1978 25\n45 1979 34\n46 1980 25\n47 1981 19\n48 1982 18\n49 1983 25\n50 1984 22\n51 1985 18\n52 1986 17\n53 1987 18\n54 1988 15\n55 1989 19\n56 1990 10\n57 1991 10\n58 1992 14\n59 1993 11\n60 1994 18\n61 1995 12\n62 1996  7\n63 1997  8\n64 1998  7\n65 1999 11\n66 2000 12\n67 2001  7\n68 2002 12\n69 2003 14\n70 2004  9\n71 2005 15\n72 2006 14\n73 2007 16\n74 2008 19\n75   NA  6\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse",
      "Frequency Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/cum-table.html",
    "href": "tutorials/frequency-tables-tidy/cum-table.html",
    "title": "Cumulative Proportion Table",
    "section": "",
    "text": "Cumulative proportions are useful when categories have a meaningful order—such as ratings or levels of satisfaction. We can compute them with cumsum():\n\nlibrary(dplyr)\nsongs %&gt;%\n  count(THEME) %&gt;%\n  mutate(\n    prop  = n / sum(n),\n    cum_prop = cumsum(prop)\n  )\n\n                 THEME   n  prop cum_prop\n1           Heartbreak 145 0.145    0.145\n2       Life_and_death 131 0.131    0.276\n3                 Love 139 0.139    0.415\n4          Party_songs 162 0.162    0.577\n5    People_and_places 145 0.145    0.722\n6 Politics_and_protest 141 0.141    0.863\n7                  Sex 131 0.131    0.994\n8                 &lt;NA&gt;   6 0.006    1.000\n\n\nThis produces a table showing the distribution and how it accumulates across categories.\nIf the categories are not already ordered, you may want to arrange() them before computing the cumulative values.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse",
      "Cumulative Proportion Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/frequency-table.html",
    "href": "tutorials/frequency-tables-baseR/frequency-table.html",
    "title": "Frequency Tables",
    "section": "",
    "text": "In 2009, The Guardian newspaper published a list entitled “1000 songs you need to hear before you die”. What kind of songs would they have selected for that? That’s exactly what we’re going to find out now!\nWe aquired a dataset containing the 1000 songs and information about them. After importing the data to R, we can look at the first rows of the dataset with the following command:\n\nhead(songs)\n\n  THEME                 TITLE          ARTIST YEAR\n1  Love      The_Look_of_Love             ABC 1982\n2  Love           The_Shining Badly_Drawn_Boy 2000\n3  Love        God_Only_Knows  The_Beach_Boys 1966\n4  Love       Good_Vibrations  The_Beach_Boys 1966\n5  Love Wouldnâ€™t_It_Be_Nice  The_Beach_Boys 1966\n6  Love     Eight_Days_a_Week     The_Beatles 1964\n                                           SPOTIFY_URL\n1 http://open.spotify.com/track/78j3qTBdzcIiT3eS7XymoD\n2 http://open.spotify.com/track/2PojSoZ94AIzp7fsz6wtMt\n3 http://open.spotify.com/track/0ObrXLrfrqJUNc8RfmIBHP\n4 http://open.spotify.com/track/2oF7FZHIJbzjeEXZ3D0Ku4\n5 http://open.spotify.com/track/0cx32rX0uZvcJUP92Wkj2y\n6                                                     \n\n\nThe dataset contains five columns for the characteristics of the song (which we also call “variables”) and 1000 rows, one row for each song.\nWe are now going to create a frequency table for the theme variable in the data set. To do this, we will use the table() function. We add the exclude = NULL argument to ensure that we can also see how many missing values there are.\n\ntable(songs$THEME, exclude = NULL)\n\n\n          Heartbreak       Life_and_death                 Love \n                 145                  131                  139 \n         Party_songs    People_and_places Politics_and_protest \n                 162                  145                  141 \n                 Sex                 &lt;NA&gt; \n                 131                    6 \n\n\nHere you see the resulting frequency table. Frequency means how often a value occurs. In this frequency table, you can see how often each theme occurs. We can for example see that there are 145 songs about “Heartbreak” and 131 songs about “Sex”.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nMake a frequency table for the variable YEAR. Use again the exclude = NULL argument.\nNOTE: The songs dataset is already loaded in the working directory of this webr session.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\ntable(songs$YEAR, exclude = NULL)\n\n\n1916 1922 1928 1929 1931 1932 1935 1936 1938 1939 1940 1941 1944 1946 1949 1950 \n   1    1    6    2    2    2    1    1    2    3    1    2    2    2    1    2 \n1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 \n   3    1    1    4    3    9    3    6   10    5   14    7   17   27   33   37 \n1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 \n  33   39   31   23   30   27   24   27   23   18   27   25   34   25   19   18 \n1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 \n  25   22   18   17   18   15   19   10   10   14   11   18   12    7    8    7 \n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 &lt;NA&gt; \n  11   12    7   12   14    9   15   14   16   19    6 \n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/cross-tab.html",
    "href": "tutorials/frequency-tables-baseR/cross-tab.html",
    "title": "Crosstables",
    "section": "",
    "text": "Next to making frequency tables of one variable, we can also look at how often specific values on two variables occur together. We use cross tablulations for this purpose. Cross tabulations use the table() command again but then on two variables:\n\ntable(songs$THEME, songs$YEAR)\n\n                      \n                       1916 1922 1928 1929 1931 1932 1935 1936 1938 1939 1940\n  Heartbreak              0    0    0    0    1    0    0    0    0    0    0\n  Life_and_death          0    0    2    0    0    0    1    0    0    0    0\n  Love                    0    0    1    0    0    0    0    0    0    1    0\n  Party_songs             0    0    0    0    0    0    0    0    1    0    0\n  People_and_places       1    1    2    0    1    1    0    1    0    1    1\n  Politics_and_protest    0    0    0    2    0    1    0    0    1    1    0\n  Sex                     0    0    1    0    0    0    0    0    0    0    0\n                      \n                       1941 1944 1946 1949 1950 1951 1952 1953 1954 1955 1956\n  Heartbreak              0    0    0    0    0    1    0    0    0    0    0\n  Life_and_death          1    0    0    1    1    0    0    0    0    0    0\n  Love                    0    0    0    0    0    0    1    0    1    1    5\n  Party_songs             0    1    0    0    0    0    0    0    0    0    0\n  People_and_places       1    0    2    0    1    2    0    1    2    1    4\n  Politics_and_protest    0    1    0    0    0    0    0    0    0    0    0\n  Sex                     0    0    0    0    0    0    0    0    1    1    0\n                      \n                       1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967\n  Heartbreak              0    2    1    0    2    2    2    6    4    9    3\n  Life_and_death          1    2    3    0    2    1    2    2    4    5    2\n  Love                    0    1    2    1    4    2    5   11    4   11    6\n  Party_songs             0    0    1    0    0    1    1    0    3    0    5\n  People_and_places       0    1    3    3    5    1    3    3    8    7    9\n  Politics_and_protest    0    0    0    1    0    0    3    5    6    1    3\n  Sex                     2    0    0    0    1    0    1    0    4    4    5\n                      \n                       1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978\n  Heartbreak              4    4    5    5    5    3    2    5    1    2    5\n  Life_and_death          4    4    4    5    6    5    5    1    1    2    2\n  Love                    7    2    3    3    6    8    6    3    1    5    4\n  Party_songs             5    0    1    0    4    2    6    4    9    6    6\n  People_and_places       8    5    0    6    2    2    4    4    1    4    2\n  Politics_and_protest    6    8    8    7    0    2    1    2    4    6    2\n  Sex                     5    8    2    4    4    2    3    4    1    2    4\n                      \n                       1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989\n  Heartbreak              4    3    2    1    3    4    1    5    2    2    1\n  Life_and_death          2    3    1    3    1    2    1    1    2    3    2\n  Love                    3    0    0    2    0    1    2    2    2    1    2\n  Party_songs            17    5    7    5   11    7    2    1    4    1    4\n  People_and_places       3    5    2    0    5    2    3    2    2    1    1\n  Politics_and_protest    4    6    2    4    1    4    7    2    2    4    5\n  Sex                     1    3    5    3    4    2    2    4    4    3    4\n                      \n                       1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000\n  Heartbreak              2    1    1    1    2    3    2    3    3    2    2\n  Life_and_death          2    4    2    2    8    0    2    3    0    0    4\n  Love                    0    0    2    1    3    3    1    0    0    0    2\n  Party_songs             2    1    2    1    1    3    0    1    1    6    1\n  People_and_places       0    1    0    2    0    1    0    0    1    1    0\n  Politics_and_protest    1    1    4    1    0    0    1    0    0    1    1\n  Sex                     3    2    3    3    4    2    1    1    2    1    2\n                      \n                       2001 2002 2003 2004 2005 2006 2007 2008\n  Heartbreak              1    1    4    4    3    2    3    3\n  Life_and_death          0    1    2    1    3    3    2    2\n  Love                    1    0    2    0    0    0    3    1\n  Party_songs             3    4    2    2    3    1    2    6\n  People_and_places       1    1    0    0    3    2    1    1\n  Politics_and_protest    0    2    1    2    3    3    4    4\n  Sex                     1    3    3    0    0    3    1    2\n\n\nIn this cross tabulation we can see which themes in songs occurred how often in which years.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Crosstables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/summary.html",
    "href": "tutorials/frequency-tables-baseR/summary.html",
    "title": "Summary",
    "section": "",
    "text": "In R, we can create frequency tables using table(), prop.table(), and cumsum().\nSometimes you can use exclude = NULL. Think carefully about what this means for your interpretation of the content!\nWe mainly use frequency tables for variables at the nominal/ordinal level.\nFrequency tables include frequencies, but proportions, percentages, and cumulative proportions/percentages can also be added.\n\nFrequency tables are mainly used to clearly display categorical variables (nominal/ordinal), especially if you want to see the specific numbers. In other cases, it is usually smarter to use summary statistics or a visualization (e.g., a histogram).\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/prop-table.html",
    "href": "tutorials/frequency-tables-baseR/prop-table.html",
    "title": "Proportion Table",
    "section": "",
    "text": "We will now continue to work with the table that we made on the previous page.\nAs a first step, we can save the table as an object for further processing:\n\ntable.theme &lt;- table(songs$THEME, exclude = NULL)\n\nIf we now use the command prop.table() on table.theme, we will see the proportions of the themes. This indicates how large the share of each theme is (with a number between 0 and 1).\n\nprop.table(table.theme)\n\n\n          Heartbreak       Life_and_death                 Love \n               0.145                0.131                0.139 \n         Party_songs    People_and_places Politics_and_protest \n               0.162                0.145                0.141 \n                 Sex                 &lt;NA&gt; \n               0.131                0.006 \n\n\nIf we multiply the proportions by 100, we will get the percentages.\n\nprop.table(table.theme)*100\n\n\n          Heartbreak       Life_and_death                 Love \n                14.5                 13.1                 13.9 \n         Party_songs    People_and_places Politics_and_protest \n                16.2                 14.5                 14.1 \n                 Sex                 &lt;NA&gt; \n                13.1                  0.6 \n\n\n\n\n\nWhat percentage of the total number of songs in the Guardian list are about ‘People_and_places’?\n\n13.114.5&lt;NA13.9\n\n\n\n\nWrong\nCorrect\nWrong\nWrong\n\n\n\n\nNow, you can try to make these two tables yourself. First make a frequency table of the variable YEAR. Save it as an object and make a proportion table using the object.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nMake a frequency table and a proportion table for the variable YEAR. Use again theexclude = NULL argument.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\ntable.year &lt;- table(songs$YEAR, exclude = NULL)\ntable.year\n\n\n1916 1922 1928 1929 1931 1932 1935 1936 1938 1939 1940 1941 1944 1946 1949 1950 \n   1    1    6    2    2    2    1    1    2    3    1    2    2    2    1    2 \n1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 \n   3    1    1    4    3    9    3    6   10    5   14    7   17   27   33   37 \n1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 \n  33   39   31   23   30   27   24   27   23   18   27   25   34   25   19   18 \n1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 \n  25   22   18   17   18   15   19   10   10   14   11   18   12    7    8    7 \n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 &lt;NA&gt; \n  11   12    7   12   14    9   15   14   16   19    6 \n\nprop.table(table.year)\n\n\n 1916  1922  1928  1929  1931  1932  1935  1936  1938  1939  1940  1941  1944 \n0.001 0.001 0.006 0.002 0.002 0.002 0.001 0.001 0.002 0.003 0.001 0.002 0.002 \n 1946  1949  1950  1951  1952  1953  1954  1955  1956  1957  1958  1959  1960 \n0.002 0.001 0.002 0.003 0.001 0.001 0.004 0.003 0.009 0.003 0.006 0.010 0.005 \n 1961  1962  1963  1964  1965  1966  1967  1968  1969  1970  1971  1972  1973 \n0.014 0.007 0.017 0.027 0.033 0.037 0.033 0.039 0.031 0.023 0.030 0.027 0.024 \n 1974  1975  1976  1977  1978  1979  1980  1981  1982  1983  1984  1985  1986 \n0.027 0.023 0.018 0.027 0.025 0.034 0.025 0.019 0.018 0.025 0.022 0.018 0.017 \n 1987  1988  1989  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999 \n0.018 0.015 0.019 0.010 0.010 0.014 0.011 0.018 0.012 0.007 0.008 0.007 0.011 \n 2000  2001  2002  2003  2004  2005  2006  2007  2008  &lt;NA&gt; \n0.012 0.007 0.012 0.014 0.009 0.015 0.014 0.016 0.019 0.006 \n\n\n\n\n\n\n\nAs you can see, the table for the variable year is not very informative due to the large number of categories of the variable. Cumulative frequency tables can help with this problem. How to make them, you will learn on the next page.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Proportion Tables"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "By finishing this module, you’ve learned how to explore your data in R using both base functions and tidyverse tools. You can now compute summary statistics, build frequency tables, handle missing values, and reshape your data with pivoting. These skills give you a clear picture of how your dataset is structured and prepare you for deeper analysis in the modules ahead.",
    "crumbs": [
      "Open-Stat-Prog",
      "Conclusion"
    ]
  },
  {
    "objectID": "conclusion.html#next-steps",
    "href": "conclusion.html#next-steps",
    "title": "Conclusion",
    "section": "Next Steps",
    "text": "Next Steps\nOften it is useful and more intuitive to provide a visual summary of your data. Check out the Basic Visualization Module for this purpose.\n\n\n Next Module",
    "crumbs": [
      "Open-Stat-Prog",
      "Conclusion"
    ]
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/index.html",
    "href": "quizzes/summarizing-data-baseR/index.html",
    "title": "Knowledge Quiz: Tutorial 1",
    "section": "",
    "text": "Note\n\n\n\n\nClick the check-mark button to check your answer.\nClick the question-mark button to see an explanation of the solution.\n\n\n\n\n\n\nWhich of the following expressions will return TRUE?\n\nTRUE(8 - 1) == 7 & 7 &gt; (14 / 2)!TRUE\"alice\" == \"bob\" & \"alice\" == \"alice\"\"alice\" == \"bob\" | \"alice\" == \"alice\"\"foo\" == \"bar\"\n\n\n\n\nTRUE: Trivially\nFALSE: 7 is equal to 7, but 7 is not strictly greater than 7\nFALSE: Trivial negation\nFALSE: “alice” is not equal to “bob”\nTRUE: “alice” is not equal to “bob”, but “alice is equal to”alice”\nFALSE: “foo” is not equal to “bar”\n\n\n\n\n\nWhat is the value of \\(x\\) in the following equation?\n\\[\nx = \\frac{3 + 3}{3}\n\\]\nRound your answer to two decimal places.\n\n\n\nAfter rounding, we get 2.00.\n\n\n\n\nUse the following help file excerpt to answer the question below.\n\n\n\n\n\n\n\n\ncor\nR Documentation\n\n\n\n\n\nCorrelation, Variance and Covariance (Matrices)\n\nUsage\n\nvar(x, y = NULL, na.rm = FALSE, use)\n\ncov(x, y = NULL, use = \"everything\",\n    method = c(\"pearson\", \"kendall\", \"spearman\"))\n\ncor(x, y = NULL, use = \"everything\",\n    method = c(\"pearson\", \"kendall\", \"spearman\"))\n\ncov2cor(V)\n\n\n\n\n\n\n\n\nWhat is the default value for the use argument of the cor() function?\n\n\"everything\"methody\"spearman\"\"kendall\"\n\n\n\nThe default value for the use argument is \"everything\".\n\nCorrect\nWrong: This is an argument name\nWrong: This is an argument name\nWrong: This is one of the possible methods\nWrong: This is one of the possible methods\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/logical.html",
    "href": "quizzes/summarizing-data-baseR/questions/logical.html",
    "title": "Question",
    "section": "",
    "text": "Which of the following expressions will return TRUE?\n\n\n\n\"foo\" != \"bar\"\n(8 + 6) &lt; (8 * 6)\n(8 - 1) == 7 & 7 &gt;= (14 / 2)\n\"alice\" == \"bob\" | \"alice\" == \"alice\"\n!(6 &gt; 7)\nTRUE\nFALSE\n\"foo\" == \"bar\"\n(8 - 1) == 7 & 7 &gt; (14 / 2)\n\"alice\" == \"bob\" | \"alice\" != \"alice\"\n\"alice\" == \"bob\" & \"alice\" == \"alice\"\n!TRUE\nTRUE & FALSE"
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/logical.html#answerlist",
    "href": "quizzes/summarizing-data-baseR/questions/logical.html#answerlist",
    "title": "Question",
    "section": "",
    "text": "\"foo\" != \"bar\"\n(8 + 6) &lt; (8 * 6)\n(8 - 1) == 7 & 7 &gt;= (14 / 2)\n\"alice\" == \"bob\" | \"alice\" == \"alice\"\n!(6 &gt; 7)\nTRUE\nFALSE\n\"foo\" == \"bar\"\n(8 - 1) == 7 & 7 &gt; (14 / 2)\n\"alice\" == \"bob\" | \"alice\" != \"alice\"\n\"alice\" == \"bob\" & \"alice\" == \"alice\"\n!TRUE\nTRUE & FALSE"
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/logical.html#answerlist-1",
    "href": "quizzes/summarizing-data-baseR/questions/logical.html#answerlist-1",
    "title": "Question",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE: “foo” is not equal to “bar”\nTRUE: 14 is strictly less than 48\nTRUE: 7 is equal to 7, and 7 is greater than or equal to 7\nTRUE: “alice” is not equal to “bob”, but “alice is equal to”alice”\nTRUE: 6 is not strictly greater than 7, and that result is negated\nTRUE: Trivially\nFALSE: Trivially\nFALSE: “foo” is not equal to “bar”\nFALSE: 7 is equal to 7, but 7 is not strictly greater than 7\nFALSE: “alice” is not equal to “bob”, and “alice” is equal to “alice”\nFALSE: “alice” is not equal to “bob”\nFALSE: Trivial negation\nFALSE: Trivially"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/index.html",
    "href": "quizzes/frequency-tables-baseR/index.html",
    "title": "Knowledge Quiz: Tutorial 2",
    "section": "",
    "text": "Note\n\n\n\n\nClick the check-mark button to check your answer.\nClick the question-mark button to see an explanation of the solution.\n\n\n\n\n\n\nWhat percentage of the total number of songs in the Guardian list are about ‘People_and_places’?\n\n14.5&lt;NA13.113.9\n\n\n\n\nCorrect\nWrong\nWrong\nWrong\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/prop-table_question.html",
    "href": "quizzes/frequency-tables-baseR/questions/prop-table_question.html",
    "title": "Question",
    "section": "",
    "text": "What percentage of the total number of songs in the Guardian list are about ‘People_and_places’?\n\n\n\n13.1\n&lt;NA\n14.5\n13.9"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/prop-table_question.html#answerlist",
    "href": "quizzes/frequency-tables-baseR/questions/prop-table_question.html#answerlist",
    "title": "Question",
    "section": "",
    "text": "13.1\n&lt;NA\n14.5\n13.9"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/prop-table_question.html#answerlist-1",
    "href": "quizzes/frequency-tables-baseR/questions/prop-table_question.html#answerlist-1",
    "title": "Question",
    "section": "Answerlist",
    "text": "Answerlist\n\nWrong\nWrong\nCorrect\nWrong"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/cumsum-question.html",
    "href": "quizzes/frequency-tables-baseR/questions/cumsum-question.html",
    "title": "Question",
    "section": "",
    "text": "Which proportion of songs was created before the year 2000?\n\n\n\n0.876\n0.888\n876\n888"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/cumsum-question.html#answerlist",
    "href": "quizzes/frequency-tables-baseR/questions/cumsum-question.html#answerlist",
    "title": "Question",
    "section": "",
    "text": "0.876\n0.888\n876\n888"
  },
  {
    "objectID": "quizzes/frequency-tables-baseR/questions/cumsum-question.html#answerlist-1",
    "href": "quizzes/frequency-tables-baseR/questions/cumsum-question.html#answerlist-1",
    "title": "Question",
    "section": "Answerlist",
    "text": "Answerlist\n\nCorrect\nWrong\nWrong\nWrong"
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/arithmetic2.html",
    "href": "quizzes/summarizing-data-baseR/questions/arithmetic2.html",
    "title": "Question",
    "section": "",
    "text": "Question\nWhat is the value of \\(x\\) in the following equation?\n\\[\nx = \\frac{3 + 1}{4}\n\\]\nRound your answer to two decimal places.\n\n\nSolution\nAfter rounding, we get 1.00.\n\n\nMeta-information\nexname: Arithmetic extype: num exsolution: 1.00 extol: 0.01\n\n\n\n\n Back to top"
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/default_arguments2.html",
    "href": "quizzes/summarizing-data-baseR/questions/default_arguments2.html",
    "title": "Question",
    "section": "",
    "text": "Use the following help file excerpt to answer the question below.\n\n\n\n\n\n\n\n\ncor\nR Documentation\n\n\n\n\n\n\n\n\n\nvar(x, y = NULL, na.rm = FALSE, use)\n\ncov(x, y = NULL, use = \"everything\",\n    method = c(\"pearson\", \"kendall\", \"spearman\"))\n\ncor(x, y = NULL, use = \"everything\",\n    method = c(\"pearson\", \"kendall\", \"spearman\"))\n\ncov2cor(V)\n\n\n\n\n\n\n\n\nWhat is the default value for the use argument of the cor() function?\n\n\n\n\"everything\"\n\"Everything\n\"spearman\"\n\"pearson\"\n\"kendall\"\nc(\"pearson\", \"spearman\", \"kendall\")\nNULL\nmethod\ny\nx\nThe use argument does not have a defined default value."
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/default_arguments2.html#answerlist",
    "href": "quizzes/summarizing-data-baseR/questions/default_arguments2.html#answerlist",
    "title": "Question",
    "section": "",
    "text": "\"everything\"\n\"Everything\n\"spearman\"\n\"pearson\"\n\"kendall\"\nc(\"pearson\", \"spearman\", \"kendall\")\nNULL\nmethod\ny\nx\nThe use argument does not have a defined default value."
  },
  {
    "objectID": "quizzes/summarizing-data-baseR/questions/default_arguments2.html#answerlist-1",
    "href": "quizzes/summarizing-data-baseR/questions/default_arguments2.html#answerlist-1",
    "title": "Question",
    "section": "Answerlist",
    "text": "Answerlist\n\nCorrect\nWrong: The “E” should be lowercase\nWrong: This is one of the possible methods\nWrong: This is one of the possible methods\nWrong: This is one of the possible methods\nWrong: These are all of the possible methods\nWrong: This is the default value for y\nWrong: This is an argument name\nWrong: This is an argument name\nWrong: This is an argument name\nWrong: There is a default value defined for use."
  },
  {
    "objectID": "in_progress.html",
    "href": "in_progress.html",
    "title": "In Progress",
    "section": "",
    "text": "Sorry, we’re still building the page you’ve requested.\nIt’s almost done…really.\nWe’re just working through a few minor issues…\n\n\n\nImage Source"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Welcome to the Descriptive Statistics module! In this module, you will learn about:\nEach tutorial will comprise a series of interactive lessons with practice problems embedded throughout.",
    "crumbs": [
      "Open-Stat-Prog",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Descriptive Statistics",
    "section": "Learning Goals",
    "text": "Learning Goals\nAfter completing this module, you will be able to:\n\nSummarizing Data\n\nCalculate the mean, the standard deviation and the range of a variable\nCreate summary statistics using the summary() function\nUse the describe() function\nAccount for missing values in your data when calculating summary statistics\n\n\n\nFrequency Tables\n\nMake frequency tables in R\nMake proportion and cumulative proportion tables\nMake readable tables including various frequency measures\nMaking cross tables between two variables\n\n\n\nSummarizing Data with the Tidyverse\n\nUse summarise() to compute descriptive statistics in a tidy, pipeline-based workflow\nApply summary functions to multiple variables at once using across()\nCreate grouped summaries with group_by()\nCompute counts, proportions, and missingness with tidyverse tools\n\n\n\nFrequency Tables with the Tidyverse\n\nUse count() to generate tidy frequency tables\nAdd proportions and cumulative proportions using mutate()\nCreate grouped or conditional frequency tables\n\nPivoting and Reshaping Data\n\nUnderstand when long or wide format is most appropriate for descriptive analyses\nUse pivot_longer() to transform repeated measures into tidy long format\nUse pivot_wider() to restructure summaries or prepare matrix-like data\nControl naming conventions and handle missing values during pivoting\n\nClick the button below to get started with the first tutorial.\n\n\n Begin Tutorial",
    "crumbs": [
      "Open-Stat-Prog",
      "Overview"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/cum-table.html",
    "href": "tutorials/frequency-tables-baseR/cum-table.html",
    "title": "Cumulative Proportion Table",
    "section": "",
    "text": "What might be clearer than the proportion table on the last page are the cumulative frequencies or proportions. This would allow us to see how many songs in total, or what proportion of all songs, were released before a certain year.\nTo do this, we use the cumsum() function on the tables. First the frequency table:\n\ntable.year &lt;- table(songs$YEAR, exclude = NULL)\ncumsum(table.year)\n\n1916 1922 1928 1929 1931 1932 1935 1936 1938 1939 1940 1941 1944 1946 1949 1950 \n   1    2    8   10   12   14   15   16   18   21   22   24   26   28   29   31 \n1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 \n  34   35   36   40   43   52   55   61   71   76   90   97  114  141  174  211 \n1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 \n 244  283  314  337  367  394  418  445  468  486  513  538  572  597  616  634 \n1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 \n 659  681  699  716  734  749  768  778  788  802  813  831  843  850  858  865 \n1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 &lt;NA&gt; \n 876  888  895  907  921  930  945  959  975  994 1000 \n\n\nWe can use the same on a proportion table:\n\ncumsum(prop.table(table.year))\n\n 1916  1922  1928  1929  1931  1932  1935  1936  1938  1939  1940  1941  1944 \n0.001 0.002 0.008 0.010 0.012 0.014 0.015 0.016 0.018 0.021 0.022 0.024 0.026 \n 1946  1949  1950  1951  1952  1953  1954  1955  1956  1957  1958  1959  1960 \n0.028 0.029 0.031 0.034 0.035 0.036 0.040 0.043 0.052 0.055 0.061 0.071 0.076 \n 1961  1962  1963  1964  1965  1966  1967  1968  1969  1970  1971  1972  1973 \n0.090 0.097 0.114 0.141 0.174 0.211 0.244 0.283 0.314 0.337 0.367 0.394 0.418 \n 1974  1975  1976  1977  1978  1979  1980  1981  1982  1983  1984  1985  1986 \n0.445 0.468 0.486 0.513 0.538 0.572 0.597 0.616 0.634 0.659 0.681 0.699 0.716 \n 1987  1988  1989  1990  1991  1992  1993  1994  1995  1996  1997  1998  1999 \n0.734 0.749 0.768 0.778 0.788 0.802 0.813 0.831 0.843 0.850 0.858 0.865 0.876 \n 2000  2001  2002  2003  2004  2005  2006  2007  2008  &lt;NA&gt; \n0.888 0.895 0.907 0.921 0.930 0.945 0.959 0.975 0.994 1.000 \n\n\n\n\n\nWhich proportion of songs was created before the year 2000?\n\n8768880.8880.876\n\n\n\n\nWrong\nWrong\nWrong\nCorrect\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Cumulative Proportion Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/index.html",
    "href": "tutorials/frequency-tables-baseR/index.html",
    "title": "Frequency Tables",
    "section": "",
    "text": "In this tutorial, you will learn:\n\nHow to create a frequency table in R\nHow to calculate proportions and cumulative proportions in a table\nHow to make your table readable\nHow to make cross tabulations of two variables\n\nWe assume that you already know:\n\nWhat measurement levels are\nHow to open a dataset (.csv) in R\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-baseR/cbind-tables.html",
    "href": "tutorials/frequency-tables-baseR/cbind-tables.html",
    "title": "Making Readable Tables",
    "section": "",
    "text": "We have now retrieved various tables (frequencies, proportions, percentages, and the cumulative sum), and we can extract the information we need from them, but they are not very readable.\nWe can also combine these different options in R into a single table. We will do this for the variable THEME. To do this, we first create all the separate tables and give them clear names:\n\nfrequencies &lt;- table(songs$THEME, exclude = NULL)\nproportions &lt;- prop.table(frequencies)\ncum_proportions &lt;- cumsum(proportions)\npercentages &lt;- prop.table(frequencies)*100\ncum_percentages &lt;- cumsum(percentages)\n\nThen we can combine these tables using the cbind() function. This ensures that each separate table fills one column in the combined table.\n\ncbind(frequencies, percentages, cum_percentages, proportions, cum_proportions)\n\n                     frequencies percentages cum_percentages proportions\nHeartbreak                   145        14.5            14.5       0.145\nLife_and_death               131        13.1            27.6       0.131\nLove                         139        13.9            41.5       0.139\nParty_songs                  162        16.2            57.7       0.162\nPeople_and_places            145        14.5            72.2       0.145\nPolitics_and_protest         141        14.1            86.3       0.141\nSex                          131        13.1            99.4       0.131\n&lt;NA&gt;                           6         0.6           100.0       0.006\n                     cum_proportions\nHeartbreak                     0.145\nLife_and_death                 0.276\nLove                           0.415\nParty_songs                    0.577\nPeople_and_places              0.722\nPolitics_and_protest           0.863\nSex                            0.994\n&lt;NA&gt;                           1.000\n\n\nNow we have everything clearly arranged side by side!\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables",
      "Readable Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/prop-table.html",
    "href": "tutorials/frequency-tables-tidy/prop-table.html",
    "title": "Proportion Tables",
    "section": "",
    "text": "Once you have the frequencies, the next step is often to compute proportions: the share of the sample represented by each category. Because count() returns a tibble, we can easily extend it using mutate():\n\nlibrary(dplyr)\nsongs %&gt;%\n  count(THEME) %&gt;%\n  mutate(prop = n / sum(n))\n\n                 THEME   n  prop\n1           Heartbreak 145 0.145\n2       Life_and_death 131 0.131\n3                 Love 139 0.139\n4          Party_songs 162 0.162\n5    People_and_places 145 0.145\n6 Politics_and_protest 141 0.141\n7                  Sex 131 0.131\n8                 &lt;NA&gt;   6 0.006\n\n\nThis creates a new column prop containing the relative frequency of each category.\nYou can also turn proportions into percentages:\n\nsongs %&gt;%\n  count(THEME) %&gt;%\n  mutate(\n    prop = n / sum(n),\n    pct  = prop * 100\n  )\n\n                 THEME   n  prop  pct\n1           Heartbreak 145 0.145 14.5\n2       Life_and_death 131 0.131 13.1\n3                 Love 139 0.139 13.9\n4          Party_songs 162 0.162 16.2\n5    People_and_places 145 0.145 14.5\n6 Politics_and_protest 141 0.141 14.1\n7                  Sex 131 0.131 13.1\n8                 &lt;NA&gt;   6 0.006  0.6\n\n\nThis approach is more explicit and easier to read than prop.table(table(x)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nMake a frequency table and a proportion table for the variable YEAR with dplyr functions.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nsongs %&gt;%\n  count(YEAR) %&gt;%\n  mutate(\n    prop = n / sum(n),\n    pct  = prop * 100\n  )\n\n   YEAR  n  prop pct\n1  1916  1 0.001 0.1\n2  1922  1 0.001 0.1\n3  1928  6 0.006 0.6\n4  1929  2 0.002 0.2\n5  1931  2 0.002 0.2\n6  1932  2 0.002 0.2\n7  1935  1 0.001 0.1\n8  1936  1 0.001 0.1\n9  1938  2 0.002 0.2\n10 1939  3 0.003 0.3\n11 1940  1 0.001 0.1\n12 1941  2 0.002 0.2\n13 1944  2 0.002 0.2\n14 1946  2 0.002 0.2\n15 1949  1 0.001 0.1\n16 1950  2 0.002 0.2\n17 1951  3 0.003 0.3\n18 1952  1 0.001 0.1\n19 1953  1 0.001 0.1\n20 1954  4 0.004 0.4\n21 1955  3 0.003 0.3\n22 1956  9 0.009 0.9\n23 1957  3 0.003 0.3\n24 1958  6 0.006 0.6\n25 1959 10 0.010 1.0\n26 1960  5 0.005 0.5\n27 1961 14 0.014 1.4\n28 1962  7 0.007 0.7\n29 1963 17 0.017 1.7\n30 1964 27 0.027 2.7\n31 1965 33 0.033 3.3\n32 1966 37 0.037 3.7\n33 1967 33 0.033 3.3\n34 1968 39 0.039 3.9\n35 1969 31 0.031 3.1\n36 1970 23 0.023 2.3\n37 1971 30 0.030 3.0\n38 1972 27 0.027 2.7\n39 1973 24 0.024 2.4\n40 1974 27 0.027 2.7\n41 1975 23 0.023 2.3\n42 1976 18 0.018 1.8\n43 1977 27 0.027 2.7\n44 1978 25 0.025 2.5\n45 1979 34 0.034 3.4\n46 1980 25 0.025 2.5\n47 1981 19 0.019 1.9\n48 1982 18 0.018 1.8\n49 1983 25 0.025 2.5\n50 1984 22 0.022 2.2\n51 1985 18 0.018 1.8\n52 1986 17 0.017 1.7\n53 1987 18 0.018 1.8\n54 1988 15 0.015 1.5\n55 1989 19 0.019 1.9\n56 1990 10 0.010 1.0\n57 1991 10 0.010 1.0\n58 1992 14 0.014 1.4\n59 1993 11 0.011 1.1\n60 1994 18 0.018 1.8\n61 1995 12 0.012 1.2\n62 1996  7 0.007 0.7\n63 1997  8 0.008 0.8\n64 1998  7 0.007 0.7\n65 1999 11 0.011 1.1\n66 2000 12 0.012 1.2\n67 2001  7 0.007 0.7\n68 2002 12 0.012 1.2\n69 2003 14 0.014 1.4\n70 2004  9 0.009 0.9\n71 2005 15 0.015 1.5\n72 2006 14 0.014 1.4\n73 2007 16 0.016 1.6\n74 2008 19 0.019 1.9\n75   NA  6 0.006 0.6\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse",
      "Proportion Tables"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/more_readable.html",
    "href": "tutorials/frequency-tables-tidy/more_readable.html",
    "title": "Making Frequency Tables More Readable",
    "section": "",
    "text": "Tidyverse frequency tables are already clean, but you can enhance readability. For example, you might rename columns:\n\nlibrary(dplyr)\nsongs %&gt;%\n  count(THEME) %&gt;%\n  rename(\n    frequency = n,\n    music_theme = THEME\n  )\n\n           music_theme frequency\n1           Heartbreak       145\n2       Life_and_death       131\n3                 Love       139\n4          Party_songs       162\n5    People_and_places       145\n6 Politics_and_protest       141\n7                  Sex       131\n8                 &lt;NA&gt;         6\n\n\nOr format percentages neatly:\n\nsongs %&gt;%\n  count(THEME) %&gt;%\n  mutate(\n    percentage = paste(n / sum(n) * 100, \"%\")\n  ) %&gt;%\n  rename(\n    frequency = n,\n    music_theme = THEME\n  )\n\n           music_theme frequency percentage\n1           Heartbreak       145     14.5 %\n2       Life_and_death       131     13.1 %\n3                 Love       139     13.9 %\n4          Party_songs       162     16.2 %\n5    People_and_places       145     14.5 %\n6 Politics_and_protest       141     14.1 %\n7                  Sex       131     13.1 %\n8                 &lt;NA&gt;         6      0.6 %\n\n\nYou can also pipe the result directly into knitr::kable() or gt::gt() for presentation-quality tables in reports.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse",
      "Making Frequency Tables More Readable"
    ]
  },
  {
    "objectID": "tutorials/frequency-tables-tidy/index.html",
    "href": "tutorials/frequency-tables-tidy/index.html",
    "title": "Frequency Tables with dplyr",
    "section": "",
    "text": "Frequency tables are one of the simplest ways to explore how values are distributed within a variable. While base R provides functions such as table() and prop.table(), the tidyverse offers a more readable and consistent approach, especially when you are already working within pipelines.\nIn this section, we will look at how to compute frequencies, proportions, cumulative proportions, and cross-tabulations using tidyverse tools.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Frequency Tables with tidyverse"
    ]
  },
  {
    "objectID": "tutorials/pivot/pivot_longer.html",
    "href": "tutorials/pivot/pivot_longer.html",
    "title": "From Wide to Long",
    "section": "",
    "text": "A dataset is considered wide when repeated measurements are spread across multiple columns. For example, imagine we have a small subset of BFI data where each person has multiple personality items stored across separate columns:\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"bfi\", package = \"psych\")\nbfi_small &lt;- bfi %&gt;% \n  mutate(id = row_number()) %&gt;%\n  select(id, A1, A2, A3)\nhead(bfi_small)\n\n      id A1 A2 A3\n61617  1  2  4  3\n61618  2  2  4  5\n61620  3  5  4  5\n61621  4  4  4  6\n61622  5  2  3  3\n61623  6  6  6  5\nThis is wide because the items A1, A2, and A3 are all separate variables, even though they represent repeated measures of the same underlying construct.\nTo convert these columns into a long format, we use pivot_longer():\nbfi_small_long &lt;- bfi_small %&gt;%\n  pivot_longer(\n    cols = A1:A3,\n    names_to = \"item\",\n    values_to = \"response\"\n  )\n\nhead(bfi_small_long)\n\n# A tibble: 6 × 3\n     id item  response\n  &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n1     1 A1           2\n2     1 A2           4\n3     1 A3           3\n4     2 A1           2\n5     2 A2           4\n6     2 A3           5\nNow each row corresponds to a single item response, the item column tells us which item it was, the response column gives the value.\nThis long structure is ideal for plotting, modeling, and group-based summaries.",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "From Wide to Long"
    ]
  },
  {
    "objectID": "tutorials/pivot/pivot_longer.html#pivoting-multiple-columns-at-once",
    "href": "tutorials/pivot/pivot_longer.html#pivoting-multiple-columns-at-once",
    "title": "From Wide to Long",
    "section": "Pivoting Multiple Columns at Once",
    "text": "Pivoting Multiple Columns at Once\nIn real datasets, multiple variables may need pivoting simultaneously. For example, suppose we have two versions of a test, with “pre” and “post” scores:\n\nlibrary(tidyr)\ndata &lt;- tibble(\n  id = 1:3,\n  pre_A = c(3, 4, 2),\n  post_A = c(4, 5, 3),\n  pre_B = c(2, 3, 2),\n  post_B = c(3, 4, 3)\n)\nhead(data)\n\n# A tibble: 3 × 5\n     id pre_A post_A pre_B post_B\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1     3      4     2      3\n2     2     4      5     3      4\n3     3     2      3     2      3\n\n\nWe can take all pre_* and post_* columns long:\n\nlibrary(dplyr)\ndata_long &lt;- data %&gt;%\n  pivot_longer(\n    cols = -id,\n    names_to = c(\"time\", \"item\"),\n    names_sep = \"_\",\n    values_to = \"score\"\n  )\nhead(data_long)\n\n# A tibble: 6 × 4\n     id time  item  score\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 pre   A         3\n2     1 post  A         4\n3     1 pre   B         2\n4     1 post  B         3\n5     2 pre   A         4\n6     2 post  A         5\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTransform the following wide dataset into long format. The dataset contains measurements of height and weight taken at two different time points (Time1 and Time2).\n\n\n# A tibble: 4 × 5\n     id height_Time1 height_Time2 weight_Time1 weight_Time2\n  &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1     1          150          152           50           52\n2     2          160          162           60           62\n3     3          170          172           70           72\n4     4          180          182           80           82\n\n\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(\n    cols = -id,\n    names_to = c(\"measurement\", \"time\"),\n    names_sep = \"_\",\n    values_to = \"value\"\n  )",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables",
      "From Wide to Long"
    ]
  },
  {
    "objectID": "tutorials/pivot/index.html",
    "href": "tutorials/pivot/index.html",
    "title": "Pivotting",
    "section": "",
    "text": "Many datasets do not arrive in the format that is easiest to analyze. Sometimes values that belong in columns are stored in rows, or vice versa. The tidyverse provides two powerful functions, pivot_longer() and pivot_wider(), that allow you to reshape data flexibly and consistently. These functions belong to the tidyr package, which is part of the tidyverse.\nPivoting is essential because most statistical and visualization tools expect data in tidy format, where each variable lives in its own column, each observation occupies one row, and each value is a single cell.\nIn this tutorial, we explore how to go from wide to long format, long to wide format, and how to control names, values, and missing data in the process.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Pivoting Tables"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/summary.html",
    "href": "tutorials/summarizing-data-baseR/summary.html",
    "title": "Summary",
    "section": "",
    "text": "In this tutorial we covered how to request descriptive statistics in R\n\nUsing mean(), min(), max(), and sd() for a single variable\nUsing summary() for an entire dataset or a selection of variables\nAnd using describe() from the psych library for slightly different descriptive statistics\n\nSummary statistics are mostly used for numeric data. If we want to summarize categorical variables, we can use frequency tables. We will have a look at this in the next tutorial of this module.\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data",
      "Summary"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/describe-function.html",
    "href": "tutorials/summarizing-data-baseR/describe-function.html",
    "title": "Using the describe function",
    "section": "",
    "text": "The summary() function provides a handy overview of descriptive statistics. There is also another function that can do the same thing, and also provides a little more information: describe() from the psych package. We need to install the psych package once and then can load it via the library command.\n\ninstall.packages(\"psych\")\n\nThe following package(s) will be installed:\n- psych [2.5.6]\nThese packages will be installed into \"~/Documenti/Utrecht/uni/varie/osp_tutorials/descriptive-statistics/renv/library/linux-tuxedo-jammy/R-4.4/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing psych ...                          OK [linked from cache]\nSuccessfully installed 1 package in 4.5 milliseconds.\n\nlibrary(psych)\n\ndescribe(stocks)\n\n          vars  n    mean    sd median trimmed   mad  min  max range  skew\nYEAR         1 88 1971.50 25.55 1971.5 1971.50 32.62 1928 2015    87  0.00\nTBONDS       2 82    5.46  8.00    4.0    4.88  5.93  -11   33    44  0.88\nSPSTOCK      3 88   11.42 19.85   14.0   12.12 20.76  -44   53    97 -0.38\nTBONDS_D     4 82    0.80  0.40    1.0    0.88  0.00    0    1     1 -1.51\nSPSTOCK_D    5 88    0.72  0.45    1.0    0.76  0.00    0    1     1 -0.94\n          kurtosis   se\nYEAR         -1.24 2.72\nTBONDS        1.10 0.88\nSPSTOCK      -0.07 2.12\nTBONDS_D      0.29 0.04\nSPSTOCK_D    -1.13 0.05\n\n\nHere we can also see, for example, the number of observations per variable (n) and the range (the difference between the minimum and maximum values observed).\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data",
      "Describe function"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-baseR/missing-values.html",
    "href": "tutorials/summarizing-data-baseR/missing-values.html",
    "title": "Missing values",
    "section": "",
    "text": "Let’s take another look at the stocks data.\n\nhead(stocks)\n\n  YEAR TBONDS SPSTOCK TBONDS_D SPSTOCK_D\n1 1928      1      44        1         1\n2 1929      4      -8        1         0\n3 1930     NA     -25       NA         0\n4 1931     -3     -44        0         0\n5 1932      9      -9        1         0\n6 1933      2      50        1         1\n\n\nYou will notice that there are missing values on the TBONDS variable. In R, missing values can be presented in different ways.\n\nStandard format: the correct way to represent a missing value is NA which is a logical class\nNon-standard formats: these are usually character strings such as \"No answer\" or \"Don’t know\"\n\nYou can easily search an entire data for missing values in standard format using anyNA(). If missing values are present this will return TRUE, else FALSE if there are no missing values.\n\nanyNA(stocks)\n\n[1] TRUE\n\n\nSo we know there is at least one missing value in stocks, but we don’t know where. You can find the position of missing values in the data using is.na(). This returns a TRUE or FALSE response rowwise and columwise.\n\nis.na(stocks)\n\n       YEAR TBONDS SPSTOCK TBONDS_D SPSTOCK_D\n [1,] FALSE  FALSE   FALSE    FALSE     FALSE\n [2,] FALSE  FALSE   FALSE    FALSE     FALSE\n [3,] FALSE   TRUE   FALSE     TRUE     FALSE\n [4,] FALSE  FALSE   FALSE    FALSE     FALSE\n [5,] FALSE  FALSE   FALSE    FALSE     FALSE\n [6,] FALSE  FALSE   FALSE    FALSE     FALSE\n [7,] FALSE  FALSE   FALSE    FALSE     FALSE\n [8,] FALSE  FALSE   FALSE    FALSE     FALSE\n [9,] FALSE  FALSE   FALSE    FALSE     FALSE\n[10,] FALSE  FALSE   FALSE    FALSE     FALSE\n[11,] FALSE  FALSE   FALSE    FALSE     FALSE\n[12,] FALSE  FALSE   FALSE    FALSE     FALSE\n[13,] FALSE  FALSE   FALSE    FALSE     FALSE\n[14,] FALSE  FALSE   FALSE    FALSE     FALSE\n[15,] FALSE   TRUE   FALSE     TRUE     FALSE\n[16,] FALSE  FALSE   FALSE    FALSE     FALSE\n[17,] FALSE  FALSE   FALSE    FALSE     FALSE\n[18,] FALSE  FALSE   FALSE    FALSE     FALSE\n[19,] FALSE  FALSE   FALSE    FALSE     FALSE\n[20,] FALSE  FALSE   FALSE    FALSE     FALSE\n[21,] FALSE  FALSE   FALSE    FALSE     FALSE\n[22,] FALSE  FALSE   FALSE    FALSE     FALSE\n[23,] FALSE   TRUE   FALSE     TRUE     FALSE\n[24,] FALSE  FALSE   FALSE    FALSE     FALSE\n[25,] FALSE  FALSE   FALSE    FALSE     FALSE\n[26,] FALSE  FALSE   FALSE    FALSE     FALSE\n[27,] FALSE  FALSE   FALSE    FALSE     FALSE\n[28,] FALSE  FALSE   FALSE    FALSE     FALSE\n[29,] FALSE  FALSE   FALSE    FALSE     FALSE\n[30,] FALSE  FALSE   FALSE    FALSE     FALSE\n[31,] FALSE  FALSE   FALSE    FALSE     FALSE\n[32,] FALSE   TRUE   FALSE     TRUE     FALSE\n[33,] FALSE  FALSE   FALSE    FALSE     FALSE\n[34,] FALSE  FALSE   FALSE    FALSE     FALSE\n[35,] FALSE  FALSE   FALSE    FALSE     FALSE\n[36,] FALSE  FALSE   FALSE    FALSE     FALSE\n[37,] FALSE  FALSE   FALSE    FALSE     FALSE\n[38,] FALSE   TRUE   FALSE     TRUE     FALSE\n[39,] FALSE  FALSE   FALSE    FALSE     FALSE\n[40,] FALSE  FALSE   FALSE    FALSE     FALSE\n[41,] FALSE  FALSE   FALSE    FALSE     FALSE\n[42,] FALSE  FALSE   FALSE    FALSE     FALSE\n[43,] FALSE  FALSE   FALSE    FALSE     FALSE\n[44,] FALSE  FALSE   FALSE    FALSE     FALSE\n[45,] FALSE  FALSE   FALSE    FALSE     FALSE\n[46,] FALSE  FALSE   FALSE    FALSE     FALSE\n[47,] FALSE  FALSE   FALSE    FALSE     FALSE\n[48,] FALSE  FALSE   FALSE    FALSE     FALSE\n[49,] FALSE  FALSE   FALSE    FALSE     FALSE\n[50,] FALSE  FALSE   FALSE    FALSE     FALSE\n[51,] FALSE  FALSE   FALSE    FALSE     FALSE\n[52,] FALSE  FALSE   FALSE    FALSE     FALSE\n[53,] FALSE  FALSE   FALSE    FALSE     FALSE\n[54,] FALSE  FALSE   FALSE    FALSE     FALSE\n[55,] FALSE  FALSE   FALSE    FALSE     FALSE\n[56,] FALSE  FALSE   FALSE    FALSE     FALSE\n[57,] FALSE  FALSE   FALSE    FALSE     FALSE\n[58,] FALSE  FALSE   FALSE    FALSE     FALSE\n[59,] FALSE  FALSE   FALSE    FALSE     FALSE\n[60,] FALSE  FALSE   FALSE    FALSE     FALSE\n[61,] FALSE  FALSE   FALSE    FALSE     FALSE\n[62,] FALSE  FALSE   FALSE    FALSE     FALSE\n[63,] FALSE  FALSE   FALSE    FALSE     FALSE\n[64,] FALSE  FALSE   FALSE    FALSE     FALSE\n[65,] FALSE  FALSE   FALSE    FALSE     FALSE\n[66,] FALSE  FALSE   FALSE    FALSE     FALSE\n[67,] FALSE  FALSE   FALSE    FALSE     FALSE\n[68,] FALSE  FALSE   FALSE    FALSE     FALSE\n[69,] FALSE  FALSE   FALSE    FALSE     FALSE\n[70,] FALSE  FALSE   FALSE    FALSE     FALSE\n[71,] FALSE  FALSE   FALSE    FALSE     FALSE\n[72,] FALSE  FALSE   FALSE    FALSE     FALSE\n[73,] FALSE  FALSE   FALSE    FALSE     FALSE\n[74,] FALSE   TRUE   FALSE     TRUE     FALSE\n[75,] FALSE  FALSE   FALSE    FALSE     FALSE\n[76,] FALSE  FALSE   FALSE    FALSE     FALSE\n[77,] FALSE  FALSE   FALSE    FALSE     FALSE\n[78,] FALSE  FALSE   FALSE    FALSE     FALSE\n[79,] FALSE  FALSE   FALSE    FALSE     FALSE\n[80,] FALSE  FALSE   FALSE    FALSE     FALSE\n[81,] FALSE  FALSE   FALSE    FALSE     FALSE\n[82,] FALSE  FALSE   FALSE    FALSE     FALSE\n[83,] FALSE  FALSE   FALSE    FALSE     FALSE\n[84,] FALSE  FALSE   FALSE    FALSE     FALSE\n[85,] FALSE  FALSE   FALSE    FALSE     FALSE\n[86,] FALSE  FALSE   FALSE    FALSE     FALSE\n[87,] FALSE  FALSE   FALSE    FALSE     FALSE\n[88,] FALSE  FALSE   FALSE    FALSE     FALSE\n\n\nBoth anyNA() and is.na() are generic methods of detecting standard missing values, and the output is not very informative. Non-standard missing values are more difficult to find because R doesn’t know that they are missing. Depending on your research question, you may want to convert non-standard missing data responses to NA for smoother data manipulation. You may also decided that these types of responses are informative and that you want to keep them as they are.\n\nAccounting for missing values when calculating summary statistics\nIf we calculate summary statistics such as the mean or the standard deviation, we have to take missing values into account.\nThe parameter na.rm in R stands for \"NA remove\" and ignores standard missing values (those that are set to NA) during calculations. By setting na.rm = TRUE, functions like mean() or sd() compute results without being affected by missing values.\nThe na.rm parameter,includes a Boolean value: TRUE or FALSE. When we set na.rm = TRUE, R excludes NA values from the calculations. Without this parameter, functions would return NA if missing values are present in the data. Take a look.\nFirst, we calculate the mean of stocks without using na.rm:\n\nmean(stocks$TBONDS)\n\n[1] NA\n\n\nAs there are missing values on the variable, the results of the calculation is NA.\nIn the next example, we add na.rm:\n\nmean(stocks$TBONDS, na.rm=TRUE)\n\n[1] 5.463415\n\n\nNow we receive our desired result, the mean of TBONDS for all available values.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTry this yourself. Request the mean of TBONDS_D using na.rm.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nmean(stocks$TBONDS_D, na.rm=TRUE)\n\n[1] 0.804878\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data",
      "Missing values"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-tidy/basic_summaries.html",
    "href": "tutorials/summarizing-data-tidy/basic_summaries.html",
    "title": "Basic Summaries",
    "section": "",
    "text": "The heart of tidyverse descriptive statistics is the summarise() function. It reduces a dataset to one or more summary statistics.\nSuppose we want to compute the mean, minimum, and maximum of the YEAR variable in the stock data:\n\nlibrary(dplyr)\nstocks %&gt;%\n  summarise(\n    mean_year = mean(YEAR, na.rm = TRUE),\n    min_year  = min(YEAR,  na.rm = TRUE),\n    max_year  = max(YEAR,  na.rm = TRUE)\n  )\n\n  mean_year min_year max_year\n1    1971.5     1928     2015\n\n\nThis code produces a one-row tibble containing the requested values.\nNotice that we must explicitly set na.rm = TRUE. Unlike base R’s summary(), tidyverse functions never remove missing values unless told to do so. This makes missing-data handling visible and intentional.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the mean of SPSTOCKS from the Stocks dataset using dplyr.\nNOTE: The songs dataset and the dplyr package are already loaded in the working directory of this webr session.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstocks %&gt;%\n  summarise(\n    mean_spstocks = mean(SPSTOCK, na.rm = TRUE)\n  )\n\n  mean_spstocks\n1      11.42045\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data with tidyverse",
      "Basic Summaries"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-tidy/across.html",
    "href": "tutorials/summarizing-data-tidy/across.html",
    "title": "Summaries Across Multiple Variables",
    "section": "",
    "text": "Often, we want to apply the same summary function to several variables. The across() helper allows us to do this in a compact and readable way.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the summary statistics for the entire stocks dataset with dplyr.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstocks %&gt;%\n  summarise(across(everything(), \n                    ~ mean(.x, na.rm = TRUE)))\n\n    YEAR   TBONDS  SPSTOCK TBONDS_D SPSTOCK_D\n1 1971.5 5.463415 11.42045 0.804878 0.7159091\n\n\n\n\n\n\n\nFor example, to compute the mean of the Bonds and the S&P500:\n\nlibrary(dplyr)\nstocks %&gt;%\n  summarise(across(c(TBONDS_D, SPSTOCK), \n                    ~ mean(.x, na.rm = TRUE)))\n\n  TBONDS_D  SPSTOCK\n1 0.804878 11.42045\n\n\nacross() selects columns and applies a function—here, mean()—to each of them.\nWe can also compute multiple summaries per variable by supplying a list of functions:\n\nstocks %&gt;%\n  summarise(across(c(TBONDS_D, SPSTOCK), \n                    list(mean = ~ mean(.x, na.rm = TRUE),\n                         sd = ~ sd(.x, na.rm = TRUE))))\n\n  TBONDS_D_mean TBONDS_D_sd SPSTOCK_mean SPSTOCK_sd\n1      0.804878   0.3987333     11.42045   19.84638\n\n\nThe result contains several columns for each variable, one for each requested statistic.\n\n\n\n\n\n\nPractice\n\n\n\nTry requesting the summary statistics for column 2 and 5 of stocks with dplyr.\n\n Interactive Editor Solution\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nstocks %&gt;%\n  summarise(across(c(2, 5), \n                    list(mean = ~ mean(.x, na.rm = TRUE),\n                         sd = ~ sd(.x, na.rm = TRUE))))\n\n  TBONDS_mean TBONDS_sd SPSTOCK_D_mean SPSTOCK_D_sd\n1    5.463415   7.99567      0.7159091    0.4535648\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data with tidyverse",
      "Summaries Across Multiple Variables"
    ]
  },
  {
    "objectID": "tutorials/summarizing-data-tidy/group_by.html",
    "href": "tutorials/summarizing-data-tidy/group_by.html",
    "title": "Grouped Summaries",
    "section": "",
    "text": "One of the strongest advantages of tidyverse summaries is their integration with grouping. Using group_by(), you can split your dataset into subgroups (such as by sex or education level) and compute summaries within each subgroup effortlessly.\nFor example, the mean of the bonds for the year befoe 1950 and after 1950 can be calculated as follows:\n\nlibrary(dplyr)\nstocks %&gt;%\n  group_by(YEAR &lt; 1950) %&gt;%\n  summarize(mean_bonds = mean(TBONDS, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  `YEAR &lt; 1950` mean_bonds\n  &lt;lgl&gt;              &lt;dbl&gt;\n1 FALSE               6.23\n2 TRUE                3.1 \n\n\nOr multiple summaries across decades:\n\nstocks %&gt;%\n  mutate(DECADE = YEAR - YEAR %% 10) %&gt;%\n  group_by(DECADE) %&gt;%\n  summarize(\n    mean_bonds = mean(TBONDS, na.rm = TRUE),\n    sd_bonds = sd(TBONDS, na.rm = TRUE),\n    mean_stocks = mean(SPSTOCK, na.rm = TRUE),\n    sd_stocks = sd(SPSTOCK, na.rm = TRUE)\n  )\n\n# A tibble: 10 × 5\n   DECADE mean_bonds sd_bonds mean_stocks sd_stocks\n    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1   1920       2.5      2.12        18        36.8\n 2   1930       3.78     3.60         4.3      33.8\n 3   1940       2.56     2.19         9.6      16.5\n 4   1950       1.38     3.20        21.1      19.8\n 5   1960       2.78     4.76         8.6      14.4\n 6   1970       5.7      6.40         7.7      19.0\n 7   1980      12.6     12.7         17.8      12.3\n 8   1990       7.7     10.2         18.7      14.0\n 9   2000       6.67     9.77         1.1      21.0\n10   2010       5        8.74        13.3      11.3\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Open-Stat-Prog",
      "Summarizing Data with tidyverse",
      "Grouped Summaries"
    ]
  }
]